{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0536ba9a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üßê Objectives üéØ</b></h1>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><strong>1. </strong>Our project has two main objectives: predicting whether a patient will cervical cancer and estimating the number of days a patient will survive with this cancer.</li>\n",
    "        <li><strong>2. </strong>To achieve these goals, we will apply machine learning algorithms for regression and classification tasks.</li>\n",
    "        <li><strong>3. </strong>We also aim to identify the most impactful features on our targets by prioritizing feature selection methods.</li>\n",
    "        <li><strong>4. </strong>We will investigate if there is a relationship between the dose rate and the duration (in years or days) from the year 1395 (Solar Year) until now.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410e260",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>ü©∫ Challenges ü§í</b></h1>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><strong>1. </strong>The dataset contains a small number of samples, posing a challenge in achieving high accuracies with various algorithms.</li>\n",
    "        <li><strong>2. </strong>Several numerical variables exhibit outliers, which can lead to potential errors, especially in KNN and SVM algorithms that are sensitive to outliers.</li>\n",
    "        <li><strong>3. </strong>The classifications target (live) has an imbalanced number of samples for class 0 and 1, resulting in bias towards the class with more samples (in this case: class 1), which can affect the algorithms' learning process.</li>\n",
    "        <li><strong>4. </strong>The presence of missing values presents a challenge. Dropping them could result in a significant loss of samples, impacting the algorithms' performance, particularly given the small dataset size.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb729c60",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üë©‚Äç‚öïÔ∏è Solutions üè•</b></h1>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><strong>1. </strong>We will apply hyper-parameter tuning to prevent overfitting or underfitting in some algorithms.</li>\n",
    "        <li><strong>2. </strong>Using RobustScaler can mitigate the impact of outliers on accuracies by scaling the data before feeding it into machine learning algorithms.</li>\n",
    "        <li><strong>3. </strong>Since the classes in the target variable are imbalanced, we will set a specific number for the random_state parameter when training the models. This helps ensure a balanced representation of both class 0 and 1 in the testing data, leading to more equitable results.</li>\n",
    "        <li><strong>4. </strong>To avoid losing more samples due to missing values, we will impute them with mean and mode values. This approach helps retain the samples in the dataset while handling missing data in a logical manner.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb965ac5",
   "metadata": {},
   "source": [
    "<a id=\"contents_tabel\"></a>\n",
    "\n",
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 15px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üìã Table of Contents</b></h1>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><a href=\"#libraries\" style=\"text-decoration: none;\">Step 1 | Import Libraries</a></li>\n",
    "        <li><a href=\"#read\" style=\"text-decoration: none;\">Step 2 | Read Dataset</a></li>\n",
    "        <li><a href=\"#description\" style=\"text-decoration: none;\">Step 3 | Description Of Variables</a></li>\n",
    "        <li><a href=\"#overview\" style=\"text-decoration: none;\">Step 4 | Overview</a></li>\n",
    "        <li><a href=\"#summary\" style=\"text-decoration: none;\">Step 5 | Statistical Summary</a></li>\n",
    "        <li><a href=\"#eda\" style=\"text-decoration: none;\">Step 6 | Exploratory Data Analysis</a>\n",
    "            <ul>\n",
    "                <li><a href=\"#univariate\" style=\"text-decoration: none;\">Step 6.1 | Univariate Analysis</a></li>\n",
    "                <li><a href=\"#bivariate\" style=\"text-decoration: none;\">Step 6.2 | Bivariate Analysis</a></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#preprocessing\" style=\"text-decoration: none;\">Step 7 | Preprocessing</a>\n",
    "            <ul>\n",
    "                <li><a href=\"#missing\" style=\"text-decoration: none;\">Step 7.1 | Handling Missing Values</a></li>\n",
    "                <li><a href=\"#duplicated\" style=\"text-decoration: none;\">Step 7.2 | Duplicated Values</a></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#corr\" style=\"text-decoration: none;\">Step 8 | Correlation Analysis</a></li>\n",
    "        <li><a href=\"#importance\" style=\"text-decoration: none;\">Step 9 | Feature Importance</a></li>\n",
    "        <li><a href=\"#modeling\" style=\"text-decoration: none;\">Step 10 | Modeling (Classification)</a>\n",
    "            <ul>\n",
    "                <li>\n",
    "                    <a href=\"#knn\" style=\"text-decoration: none;\">Step 10.1 | K-Nearest Neighbors (KNN)</a>\n",
    "                    <ul>\n",
    "                        <li><a href=\"#knn-hpt\" style=\"text-decoration: none;\">Step 10.1.1 | KNN Hyper-parameter tuning</a></li>\n",
    "                        <li><a href=\"#knn-evaluation\" style=\"text-decoration: none;\">Step 10.1.2 | KNN Evaluation</a></li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <a href=\"#svm\" style=\"text-decoration: none;\">Step 10.2 | Support Vector Machine (SVM)</a>\n",
    "                    <ul>\n",
    "                        <li><a href=\"#svm-hpt\" style=\"text-decoration: none;\">Step 10.2.1 | SVM Hyper-parameter tuning</a></li>\n",
    "                        <li><a href=\"#svm-evaluation\" style=\"text-decoration: none;\">Step 10.2.2 | SVM Evaluation</a></li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <a href=\"#nb\" style=\"text-decoration: none;\">Step 10.3 | Naive Bayes (NB)</a>\n",
    "                    <ul>\n",
    "                        <li><a href=\"#nb-hpt\" style=\"text-decoration: none;\">Step 10.3.1 | NB Hyper-parameter tuning</a></li>\n",
    "                        <li><a href=\"#nb-evaluation\" style=\"text-decoration: none;\">Step 10.3.2 | NB Evaluation</a></li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <a href=\"#dts\" style=\"text-decoration: none;\">Step 10.4 | Decision Trees (DTs)</a>\n",
    "                    <ul>\n",
    "                        <li><a href=\"#dts-hpt\" style=\"text-decoration: none;\">Step 10.4.1 | DTs Hyper-parameter tuning</a></li>\n",
    "                        <li><a href=\"#dts-evaluation\" style=\"text-decoration: none;\">Step 10.4.2 | DTs Evaluation</a></li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <a href=\"#rf\" style=\"text-decoration: none;\">Step 10.5 | Random Forest (RF)</a>\n",
    "                    <ul>\n",
    "                        <li><a href=\"#rf-hpt\" style=\"text-decoration: none;\">Step 10.5.1 | RF Hyper-parameter tuning</a></li>\n",
    "                        <li><a href=\"#rf-evaluation\" style=\"text-decoration: none;\">Step 10.5.2 | RF Evaluation</a></li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <a href=\"#mlp\" style=\"text-decoration: none;\">Step 10.6 | Multilayer Perpectron (MLP)</a>\n",
    "                    <ul>\n",
    "                        <li><a href=\"#mlp-hpt\" style=\"text-decoration: none;\">Step 10.6.1 | MLP Hyper-parameter tuning</a></li>\n",
    "                        <li><a href=\"#mlp-evaluation\" style=\"text-decoration: none;\">Step 10.6.2 | MLP Evaluation</a></li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#survival\" style=\"text-decoration: none;\">Step 11 | C-Index, ROC-AUC</a></li>\n",
    "        <li><a href=\"#regression\" style=\"text-decoration: none;\">Step 12 | Modeling (Regression)</a></li>\n",
    "        <li><a href=\"#conc\" style=\"text-decoration: none;\">Step 13 | Conclusion</a></li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6425be6",
   "metadata": {},
   "source": [
    "<a id=\"libraries\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 1 | Import Libraries</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d53e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "\n",
    "# KNN \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# SVM \n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "# Naive bayes\n",
    "from sklearn.naive_bayes import ComplementNB, GaussianNB\n",
    "\n",
    "# Decision Tree \n",
    "from sklearn.tree import DecisionTreeClassifier ,DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "# Random Forest \n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, make_scorer\n",
    "\n",
    "# MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Color for confusion matrices\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# regression\n",
    "from sklearn.linear_model import HuberRegressor, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# tools\n",
    "from itertools import product\n",
    "\n",
    "# C-index \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690cba3",
   "metadata": {},
   "source": [
    "<a id=\"read\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 2 | Read Dataset</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9826a813",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/cervical/uterus.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/cervical/uterus.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/cervical/uterus.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/cervical/uterus.csv\")\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0323b874",
   "metadata": {},
   "source": [
    "<a id=\"description\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 3 | Description Of Variabes</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a9185",
   "metadata": {},
   "source": [
    "| **Variable** | **Description** |\n",
    "|--------------|------------------|\n",
    "| **live**     | Survival status |\n",
    "| **days**     | Days survived after cervical cancer treatment |\n",
    "| **dose rate** | The remaining dose of the brachytherapy device |\n",
    "| **stage**    | Disease stage (numeric) |\n",
    "| **stage2**   | Disease stage (alphabetical) |\n",
    "| **EQD2/HRCTV** | Equivalent dose to the tumor volume |\n",
    "| **EQ/REC**   | Equivalent dose to the rectum |\n",
    "| **EQ/BLD**   | Equivalent dose to the bladder |\n",
    "| **EQ/SIG**   | Equivalent dose to the sigmoid colon |\n",
    "| **POINT A/L** | Dose to the left pelvic sidewall at point A to the uterus |\n",
    "| **POINT A/R** | Dose to the right pelvic sidewall at point A to the uterus |\n",
    "| **VOLUM**    | Tumor volume |\n",
    "| **TX duration** | Duration of patient's radiotherapy in minutes |\n",
    "| **age**      | Patient's age |\n",
    "| **chemo**    | Whether chemotherapy was administered or not |\n",
    "| **marrage**  | Marital status |\n",
    "| **surgery**  | Related surgery involving uterus removal |\n",
    "| **smoke**    | Smoking status |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12731324",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    \"dose rate\":\"dose_rate\",\n",
    "    \"EQD2/HRCTV\":\"EQD2_HRCTV\",\n",
    "    \"EQ/REC\":\"EQ_REC\",\n",
    "    \"EQ/BLD\":\"EQ_BLD\",\n",
    "    \"EQ/SIG\":\"EQ_SIG\",\n",
    "    \"POINT A/L\":\"POINT_A_L\",\n",
    "    \"POINT A/R\":\"POINT_A_R\",\n",
    "    \"TX duration\":\"TX_duration\"    \n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a29a3",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 4 | Overiew</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c11f22",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140;\"></p>\n",
    " <b>stage2</b> column is an extra column. So, we should drop it first.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"stage2\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98174df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67414bc",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140;\"></p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ed809",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc033f3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140;\">There are 142 samples and 17 features in this dataset, contains 2 targets.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531fd7fe",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 5 | Statistical Summary</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2111c5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140;\"></p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56283a2e",
   "metadata": {},
   "source": [
    "<a id=\"eda\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 6 | Exploratary Data Analysis (EDA)</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525fbfc",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140;\">This is a dataframe before handling missing values to see the distribution of features.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3ac32",
   "metadata": {},
   "source": [
    "<a id=\"univariate\"></a>\n",
    "# <p style=\"background-color: #A45EE5; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 6.1 | Univariate Analysis (EDA)</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ade88",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"live\", \"chemo\", \"marrage\", \"surgery\", \"smoke\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_chart_plot(features, df):\n",
    "    plt.rcParams['axes.facecolor'] = '#FAE6FA'\n",
    "    \n",
    "    num_features = len(features)\n",
    "    num_cols = min(num_features, 5)\n",
    "    num_rows = 1\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        col = i % num_cols\n",
    "        \n",
    "        df[feature].value_counts().plot.pie(ax=axes[col], autopct='%1.1f%%', startangle=90, colors=['#F99BB0', '#D166B8'])\n",
    "        axes[col].set_title(f'Pie Chart for {feature}')\n",
    "        axes[col].set_ylabel('')\n",
    "        axes[col].legend()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_bar_plot(features, df):\n",
    "    plt.rcParams['axes.facecolor'] = '#FAE6FA'\n",
    "    \n",
    "    num_features = len(features)\n",
    "    num_cols = 1\n",
    "    num_rows = 2\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 12))\n",
    "    \n",
    "    colors = ['#6A0DAD', '#8E3A80', '#B656AF', '#D166B8', '#F487C1', '#F99BB0', '#FFB6A9', '#FFCDA3', '#FFDD9D']\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        sns.countplot(y=feature, data=df, ax=axes[i], palette=colors)\n",
    "        axes[i].set_title(f'Bar Chart for {feature}')\n",
    "        axes[i].set_xlabel('Count')\n",
    "        axes[i].set_ylabel('')\n",
    "        \n",
    "        for p in axes[i].patches:\n",
    "            width = p.get_width()\n",
    "            axes[i].annotate(f'{width}', (width, p.get_y() + p.get_height() / 2.),\n",
    "                             ha='center', va='center', fontsize=10, color='black', xytext=(15, 0),\n",
    "                             textcoords='offset points')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_chart_plot(cat_features, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features2 = [\"dose_rate\",\"stage\"]\n",
    "vertical_bar_plot(cat_features2, df_eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4d649",
   "metadata": {},
   "source": [
    "<a id=\"bivariate\"></a>\n",
    "# <p style=\"background-color: #A45EE5; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 6.2 | Bivariate Analysis (EDA)</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96652c6d",
   "metadata": {},
   "source": [
    "<a id=\"cls-tg\"></a>\n",
    "# <p style=\"background-color: #F487C1; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 6.2.1 | Classification Target</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_chart_target(feature, figsize):\n",
    "    colors = ['#8E3A80', '#F487C1', '#6A0DAD', '#B656AF', '#D166B8', '#F99BB0', '#FFB6A9', '#FFCDA3', '#FFDD9D']\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=figsize)\n",
    "\n",
    "    sns.countplot(y='live', hue=feature, data=df_eda, ax=axes, palette=colors)\n",
    "    axes.set_title('Bar Chart of Target vs. ' + feature)\n",
    "    axes.set_xlabel('Frequency')\n",
    "    axes.set_ylabel('Target')\n",
    "\n",
    "    total = len(df_eda[feature])\n",
    "    for p in axes.patches:\n",
    "        if p.get_width() != 0:  \n",
    "            percentage = '{:.1f}%'.format(100 * p.get_width() / total)\n",
    "            axes.annotate(f'{int(p.get_width())} ({percentage})', (p.get_width() + 0.1, p.get_y() + p.get_height() / 2), \n",
    "                          ha='left', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2970b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_eda = [\"dose_rate\", \"EQD2_HRCTV\", \"EQ_REC\", \"EQ_BLD\", \"EQ_SIG\", \"POINT_A_R\",\"POINT_A_L\", \"VOLUM\", \"TX_duration\", \"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kde_hist_plot(features, target, df):\n",
    "    plt.rcParams['axes.facecolor'] = '#FAE6FA'\n",
    "\n",
    "    num_features = len(features)\n",
    "    num_cols = min(num_features, 5)\n",
    "    num_rows = (num_features + num_cols - 1) // num_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        \n",
    "        sns.histplot(df[df[target] == 0][feature], kde=True, color='#6A0DAD', label='Class 0', ax=axes[row, col])\n",
    "        sns.histplot(df[df[target] == 1][feature], kde=True, color='#D166B8', label='Class 1', ax=axes[row, col])\n",
    "        \n",
    "        axes[row, col].set_title(f\"KDE for {feature}\")\n",
    "        axes[row, col].set_xlabel(feature)\n",
    "        axes[row, col].set_ylabel(\"Density\")\n",
    "        axes[row, col].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_chart_target(features, figsize):\n",
    "    num_features = len(features)\n",
    "    num_cols = 1\n",
    "    num_rows = min(num_features, 3)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figsize)\n",
    "    axes = axes if num_features > 1 else [axes]  \n",
    "    \n",
    "    colors = ['#880085', '#FF00FF', '#6A0DAD', '#B656AF', '#D166B8', '#F99BB0', '#E0B0FF', '#FFCDA3', '#C54B8C', \"#9370DB\", \"#301934\"]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if num_features > 1:\n",
    "            ax = axes[i]\n",
    "        else:\n",
    "            ax = axes[0]  # Single feature case\n",
    "        \n",
    "        sns.countplot(x='live', hue=feature, data=df_eda, ax=ax, palette=colors)\n",
    "        ax.set_title(f\"{feature} vs live\")\n",
    "        ax.set_xlabel('Target')\n",
    "        ax.set_ylabel('Frequency')\n",
    "\n",
    "        total = len(df_eda[feature])\n",
    "        for p in ax.patches:\n",
    "            if p.get_height() != 0:  \n",
    "                ax.annotate(f'{int(p.get_height())}', \n",
    "                            (p.get_x() + p.get_width() / 2., p.get_height() + 0.1), \n",
    "                            ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_with_target(features, target, df):\n",
    "    plt.rcParams['axes.facecolor'] = '#FAE6FA'\n",
    "\n",
    "    num_features = len(features)\n",
    "    num_cols = min(num_features, 5)\n",
    "    num_rows = 2\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n",
    "\n",
    "    colors = {0: '#6A0DAD', 1: '#D166B8'}  # Colors for box plot\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        ax = axes[row, col] if num_rows > 1 else axes[col]\n",
    "        \n",
    "        sns.boxplot(x=target, y=feature, data=df, ax=ax, palette=colors)\n",
    "        ax.set_title(f\"{feature} with {target}\")\n",
    "        ax.set_xlabel(target)\n",
    "        ax.set_ylabel(feature)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d62c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cat_features_with_target(data, target_col, cat_cols):\n",
    "    colors = ['#880085', '#FF00FF']\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "    \n",
    "    for i, col in enumerate(cat_cols):\n",
    "        sns.countplot(x=col, hue=target_col, data=data, ax=axes[i//2, i%2], palette=colors)\n",
    "        axes[i//2, i%2].set_title(f'{col} distribution by {target_col}')\n",
    "        axes[i//2, i%2].set_xlabel(col)\n",
    "        axes[i//2, i%2].set_ylabel('Count')\n",
    "        axes[i//2, i%2].legend(title=target_col, loc='upper right')\n",
    "        axes[i//2, i%2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        for p in axes[i//2, i%2].patches:\n",
    "            axes[i//2, i%2].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                                      ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), \n",
    "                                      textcoords='offset points')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_barplot_features_with_target(data, target_col, barplot_cols):\n",
    "    colors = ['#6A0DAD', '#D166B8']\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "    \n",
    "    for i, col in enumerate(barplot_cols):\n",
    "        sns.countplot(x=target_col, hue=col, data=data, ax=axes[i//2, i%2], palette=colors)\n",
    "        axes[i//2, i%2].set_title(f'{col} vs {target_col}')\n",
    "        axes[i//2, i%2].set_xlabel(target_col)\n",
    "        axes[i//2, i%2].set_ylabel('Count')\n",
    "        axes[i//2, i%2].legend(title=col)\n",
    "        \n",
    "        for p in axes[i//2, i%2].patches:\n",
    "            height = p.get_height() if not pd.isna(p.get_height()) else 0\n",
    "            axes[i//2, i%2].annotate(f'{height}', (p.get_x() + p.get_width() / 2., height), \n",
    "                                      ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), \n",
    "                                      textcoords='offset points')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b95bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_num_features_with_target(data, target_col, num_cols):\n",
    "    num_features = len(num_cols)\n",
    "    num_rows = (num_features // 3) + int(num_features % 3 != 0)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(12, 5*num_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(num_cols):\n",
    "        sns.scatterplot(x=col, y='days', hue=target_col, data=data, ax=axes[i], alpha=0.5)\n",
    "        axes[i].set_title(f'{col} vs days')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('days')\n",
    "        handles, labels = axes[i].get_legend_handles_labels()\n",
    "        axes[i].legend(handles, ['0', '1'], title=target_col)\n",
    "    \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f196d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_hist_plot(features_eda, \"live\", df_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e1da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features = [\"days\", \"dose_rate\", \"stage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5263029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_chart_target(imp_features, (14, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537349fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_features_with_target(data, 'live', ['chemo', 'marrage', 'surgery', 'smoke'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78cbbf5",
   "metadata": {},
   "source": [
    "<a id=\"reg-tg\"></a>\n",
    "# <p style=\"background-color: #F487C1; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 6.2.2 | Regression Target</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1bb8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_features_with_target(df_eda, 'live', ['dose_rate', 'age', \"EQD2_HRCTV\", \"EQ_REC\", \"EQ_BLD\", \"EQ_SIG\", \"POINT_A_L\", \"POINT_A_R\", \"VOLUM\", \"TX_duration\", \"stage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barplot_features_with_target(data, 'days', ['chemo', 'marrage', 'surgery', 'smoke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689462fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda[\"dose_rate\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "dose_rate_1 = [79.05, 68.77, 59.80]\n",
    "dose_rate_2 = [52.05, 45.28, 39.40]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "scatter1 = axes[0].scatter(df_eda[df_eda['dose_rate'].isin(dose_rate_1)]['TX_duration'], \n",
    "                           df_eda[df_eda['dose_rate'].isin(dose_rate_1)]['days'], \n",
    "                           s=df_eda[df_eda['dose_rate'].isin(dose_rate_1)]['VOLUM']*5,  # Adjust size here\n",
    "                           c=df_eda[df_eda['dose_rate'].isin(dose_rate_1)]['live'], \n",
    "                           cmap=ListedColormap(['#6A0DAD', '#D166B8']),\n",
    "                           alpha=0.5)\n",
    "axes[0].set_title('From 1395 to 1397')\n",
    "axes[0].set_xlabel('TX Duration')\n",
    "axes[0].set_ylabel('Days')\n",
    "\n",
    "scatter2 = axes[1].scatter(df_eda[df_eda['dose_rate'].isin(dose_rate_2)]['TX_duration'], \n",
    "                           df_eda[df_eda['dose_rate'].isin(dose_rate_2)]['days'], \n",
    "                           s=df_eda[df_eda['dose_rate'].isin(dose_rate_2)]['VOLUM']*5,  # Adjust size here\n",
    "                           c=df_eda[df_eda['dose_rate'].isin(dose_rate_2)]['live'], \n",
    "                           cmap=ListedColormap(['#6A0DAD', '#D166B8']),\n",
    "                           alpha=0.5)\n",
    "axes[1].set_title('From 1398 to 1400')\n",
    "axes[1].set_xlabel('TX Duration')\n",
    "axes[1].set_ylabel('Days')\n",
    "\n",
    "legend_handles = [mpatches.Patch(color='#6A0DAD', label='Live: 0'),\n",
    "                  mpatches.Patch(color='#D166B8', label='Live: 1')]\n",
    "\n",
    "axes[0].legend(handles=legend_handles, title='Live', loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "axes[1].legend(handles=legend_handles, title='Live', loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dose_rates = df_eda['dose_rate'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, dose_rate in enumerate(unique_dose_rates):\n",
    "    filtered_df = df_eda[df_eda['dose_rate'] == dose_rate]\n",
    "\n",
    "    scatter = axes[i].scatter(filtered_df['TX_duration'], \n",
    "                              filtered_df['days'], \n",
    "                              s=filtered_df['VOLUM']*5, \n",
    "                              c=filtered_df['live'], \n",
    "                              cmap=ListedColormap(['#6A0DAD', '#D166B8']),\n",
    "                              alpha=0.5)\n",
    "    \n",
    "    axes[i].set_title(f'Dose Rate: {dose_rate}')\n",
    "    axes[i].set_xlabel('TX Duration')\n",
    "    axes[i].set_ylabel('Days')\n",
    "    \n",
    "    legend_handles = [mpatches.Patch(color='#6A0DAD', label='Live: 0'),\n",
    "                      mpatches.Patch(color='#D166B8', label='Live: 1')]\n",
    "    axes[i].legend(handles=legend_handles, title='Live', loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7b77a",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 7 | Preprocessing</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b496cf",
   "metadata": {},
   "source": [
    "<a id=\"missing\"></a>\n",
    "# <p style=\"background-color: #A45EE5; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 7.1 | Handling Missing Values</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc603d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_bar_chart(df):\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_percentages = (missing_counts / len(df)) * 100\n",
    "\n",
    "    missing_df = pd.DataFrame({'Missing Count': missing_counts, 'Missing Percentage': missing_percentages})\n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0]  # Exclude columns with no missing values\n",
    "\n",
    "    missing_df.sort_values(by='Missing Count', ascending=False, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = missing_df['Missing Count'].plot(kind='barh', color='#8E3A80')\n",
    "    ax.set_xlabel('Missing Count')\n",
    "    ax.set_title('Missing Values Count and Percentage')\n",
    "\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        col_name = missing_df.index[i]\n",
    "        percentage = '{:.1f}%'.format(missing_df.loc[col_name, 'Missing Percentage'])\n",
    "        ax.annotate(f'{int(p.get_width())}\\n({percentage})', (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                    ha='left', va='center', color='black')\n",
    "\n",
    "    plt.ylabel('Columns')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "missing_values_bar_chart(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76097002",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_indexes = df[df.isnull().any(axis=1)].index\n",
    "df_missing = df.loc[missing_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17a904",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140;\"></p>\n",
    "Before filling missing values:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2d2bf",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140;\"></p>\n",
    "Filling missing values using average for age, EQ_SIG, POINT_A_L, POINT_A_R, VOLUM:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df['age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94915a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].fillna(round(df['age'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19693ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EQ_SIG'].fillna(df['EQ_SIG'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f9295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POINT_A_L'].fillna(df['POINT_A_L'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3860e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POINT_A_R'].fillna(df['POINT_A_R'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VOLUM'].fillna(df['VOLUM'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7787b7cf",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140;\"></p>\n",
    "Filling missing values using mode for chemo, marrage, surgery, smoke:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf599a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['chemo'].fillna(df['chemo'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['marrage'].fillna(df['marrage'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['surgery'].fillna(df['surgery'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205452fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['smoke'].fillna(df['smoke'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81651d2d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140;\"></p>\n",
    "After filling missing values:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c93eb5",
   "metadata": {},
   "source": [
    "<a id=\"corr\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 8 | Correlation Analysis</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71329543",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "\n",
    "custom_colors = ['#6A0DAD', '#8E3A80', '#B656AF', '#D166B8', '#F487C1', '#F99BB0', '#FFB6A9', '#FFCDA3', '#FFDD9D']\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(corr_matrix, cmap=sns.color_palette(custom_colors), annot=True)\n",
    "plt.title('Correlation Heatmap with Custom Colors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e065262",
   "metadata": {},
   "source": [
    "<a id=\"importance\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 9 | Feature Importance</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db95650",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b><b>Why Random Forest for Feature Importance Analysis?</b></b></h2>\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.5em;\">\n",
    "        <ul style=\"font-size:18px;line-height: 1.5em; font-family:calibri;\">\n",
    "            <li><strong>Robustness against overfitting:</strong> Unlike individual decision trees, Random Forest is less prone to overfitting. This means it can generalize better to unseen data, leading to more reliable feature importance estimates.</li>\n",
    "            <li><strong>Ability to understand complex relationships:</strong> Random Forest can capture interactions between features, making it suitable for identifying important features even in scenarios where features interact in intricate ways. This is particularly useful in real-world datasets where feature interactions are common.</li>\n",
    "            <li><strong>Insight into feature importance:</strong> In addition to identifying important features, Random Forest also provides a measure of their relative importance. This information helps prioritize features and focus attention on those that have the most significant impact on the classification task.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deefb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imp = df.drop([\"live\", \"days\"], axis=1)\n",
    "y_imp = df[\"live\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf940f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "rf_clf.fit(X_imp, y_imp)\n",
    "\n",
    "importances = rf_clf.feature_importances_\n",
    "feature_names = X_imp.columns\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "sorted_feature_names = [feature_names[i] for i in indices]\n",
    "sorted_importances = importances[indices]\n",
    "\n",
    "custom_colors = ['#6A0DAD', '#8E3A80', '#B656AF', '#D166B8', '#F487C1', '#F99BB0', '#FFB6A9', '#FFCDA3', '#FFDD9D']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances (Random Forest)\")\n",
    "bars = plt.barh(range(X_imp.shape[1]), sorted_importances, align=\"center\", color=custom_colors)\n",
    "plt.yticks(range(X_imp.shape[1]), sorted_feature_names)\n",
    "\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{sorted_importances[i]*100:.2f}%', va='center', ha='left', color='black')\n",
    "\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e38c6",
   "metadata": {},
   "source": [
    "<a id=\"modeling\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 10 | Modeling (Classification)</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c2f6a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">This notebook demonstrates the application of six classification machine learning algorithms, including KNN (K-Nearest Neighbors), SVM (Support Vector Machine), NB (Naive Bayes), DT (Decision Tree), and RF (Random Forest).</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf18b221",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\"><b>Note</b>: In this project, I set random state to 31 to see the performance of different algorithms on equal or near to equal number of unseen (or test) data, in our dataset -> 22 samples for class 1 and 21 samples for class 0. Also the test size for all of them is 30% of data and the cross validation for all of them set to 5 (Except Naive Bayes -> cross validation = 7).</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5d4b4",
   "metadata": {},
   "source": [
    "<a id=\"importance\"></a>\n",
    "# <p style=\"background-color: #A45EE5; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Feature Scaling</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e3ec2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b><b>Why Use Robust Scaler for Feature Scaling?</b></b></h2>\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.5em;\">\n",
    "      RobustScaler is a data preprocessing technique used in machine learning to scale numerical features while handling outliers effectively. Unlike StandardScaler and MinMaxScaler, RobustScaler is designed to be resistant to the influence of outliers in the data.\n",
    "        <ul style=\"font-size:18px;line-height: 1.5em; font-family:calibri;\">\n",
    "            <li><strong>Robustness against outliers:</strong> The Robust Scaler is less sensitive to outliers compared to other scaling methods like Min-Max Scaling. It scales features based on median and interquartile range, making it robust in the presence of outliers.</li>\n",
    "            <li><strong>Preservation of distribution shape:</strong> Unlike Min-Max Scaling, which can distort the distribution of features, Robust Scaler preserves the shape of the distribution, making it suitable for datasets with non-normal distributions.</li>\n",
    "            <li><strong>Resilience to small datasets:</strong> Robust Scaler is effective even with small datasets because it calculates scaling parameters based on robust statistics, such as median and interquartile range, which are less affected by small sample sizes.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67ab27",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "<a href=\"https://proclusacademy.com/blog/robust-scaler-outliers/\">Source for Robust Scaler to read<a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8294ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = df.copy()\n",
    "df_scaled.drop(\"days\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0293db49",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">You can see the features that I've Scaled using robust scaler in the <b>features_to_scale</b> list.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ed0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_scale = [\"dose_rate\", \"stage\", \"EQD2_HRCTV\", \"EQ_REC\", \"EQ_BLD\", \"EQ_SIG\", \"POINT_A_L\", \"POINT_A_R\", \"VOLUM\", \"TX_duration\", \"age\", \"smoke\", \"surgery\", \"marrage\", \"chemo\"]\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "features_subset = df_scaled[features_to_scale]\n",
    "\n",
    "scaled_features = scaler.fit_transform(features_subset)\n",
    "\n",
    "df_scaled[features_to_scale] = scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a40b7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">Features after applying scaling:</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c245f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36533c45",
   "metadata": {},
   "source": [
    "<a id=\"knn\"></a>\n",
    "# <p style=\"background-color: #A45EE5; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.1 | K-Nearest Neighbors (KNN)</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0f408",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>Challenges and Strengths of K-Nearest Neighbors (KNN) in our project:</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.5em;\">\n",
    "        <ul style=\"font-size:18px;line-height: 1.5em; font-family:calibri;\">\n",
    "            <li><b>Picking the Right \\( k \\):</b> Selecting the appropriate number of neighbors can be challenging. Too few neighbors may lead to overfitting, while too many may result in oversimplification.</li>\n",
    "            <li><b>Imbalanced Data:</b> KNN may be biased towards the majority class in datasets where one class is significantly more prevalent than others.</li>\n",
    "            <li><b>Outliers Mess Things Up:</b> Outliers can significantly impact KNN's decision-making process and lead to inaccurate predictions.</li>\n",
    "            <li><b>Choosing the Right Distance:</b> Different distance metrics may yield different results, and selecting the appropriate metric is crucial for KNN's performance.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.5em;\">\n",
    "        <b>Strengths of K-Nearest Neighbors (KNN):</b>\n",
    "        <ul style=\"font-size:18px;line-height: 1.5em; font-family:calibri;\">\n",
    "            <li><b>Simple and Intuitive:</b> KNN is easy to understand and implement, making it a good choice for initial exploration of a dataset or as a baseline model.</li>\n",
    "            <li><b>Non-parametric:</b> KNN doesn't make any assumptions about the underlying data distribution, making it suitable for both linear and nonlinear relationships.</li>\n",
    "            <li><b>Robust to Noisy Data:</b> KNN can handle noisy or irrelevant features well, as it relies on the proximity of data points rather than the specific values of features.</li>\n",
    "            <li><b>Doesn't Require Training:</b> Unlike many other machine learning algorithms, KNN doesn't require a training phase, which can save time and computational resources.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe39e1",
   "metadata": {},
   "source": [
    "<a id=\"importance\"></a>\n",
    "# <p style=\"background-color: #A45EE5; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Split data into Train & Test</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5921f0ca",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">Robust Scaling Applied on KNN.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eaa1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26644f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_knn = df_scaled.drop([\"live\"], axis=1)\n",
    "y_knn = df_scaled[\"live\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52987738",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_knn_train, X_knn_test, y_knn_train, y_knn_test = train_test_split(X_knn, y_knn, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31062772",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_knn_train), len(X_knn_test), len(y_knn_train), len(y_knn_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df40b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_knn_test).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad497999",
   "metadata": {},
   "source": [
    "<a id=\"knn-hpt\"></a>\n",
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.1.1 | KNN Hyper-parameter tuning</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dea9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning(train_accuracies, test_accuracies, k, X_train, X_test, y_train, y_test, p):\n",
    "    best_conf_matrices = []\n",
    "    \n",
    "    for n in np.arange(2, 12):\n",
    "        knn_classifier = KNeighborsClassifier(n_neighbors=n, p=p)\n",
    "        \n",
    "        knn_classifier.fit(X_train, y_train)\n",
    "        train_accuracy = knn_classifier.score(X_train, y_train)\n",
    "        test_accuracy = knn_classifier.score(X_test, y_test)\n",
    "        \n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        k.append(n)\n",
    "        \n",
    "        y_pred = knn_classifier.predict(X_test)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        best_conf_matrices.append(conf_matrix)\n",
    "                \n",
    "    return train_accuracies, test_accuracies, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac90f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies_p1, test_accuracies_p1, k_p1 = [], [], []\n",
    "train_accuracies_p2, test_accuracies_p2, k_p2 = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e95ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies_p1, test_accuracies_p1, k_p1 = tuning(train_accuracies_p1, test_accuracies_p1, k_p1, X_knn_train, X_knn_test, y_knn_train, y_knn_test, p=1)\n",
    "train_accuracies_p2, test_accuracies_p2, k_p2 = tuning(train_accuracies_p2, test_accuracies_p2, k_p2, X_knn_train, X_knn_test, y_knn_train, y_knn_test, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f4b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_knn_evaluation_2x2(train_accuracies_list, test_accuracies_list, k_list, df_name_list, p_list):\n",
    "    \"\"\"\n",
    "    This function plots the results of train and test accuracies for KNN models\n",
    "    with different values of \"p\" in a 2x2 grid of subplots.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    colors = [['green', 'purple'], ['red', 'blue']] \n",
    "\n",
    "    for i in range(len(train_accuracies_list)):\n",
    "        axs[i].plot(k_list[i], train_accuracies_list[i], label='Training Accuracy', marker='o', color=colors[i][0])\n",
    "        axs[i].plot(k_list[i], test_accuracies_list[i], label='Test Accuracy', marker='x', color=colors[i][1])\n",
    "        \n",
    "        axs[i].set_title(f'KNN Accuracy, p={p_list[i]}, {df_name_list[i]}')\n",
    "        axs[i].set_xlabel('Number of Neighbors')\n",
    "        axs[i].set_ylabel('Accuracy')\n",
    "        axs[i].legend()\n",
    "        axs[i].grid(True)\n",
    "        axs[i].set_xticks(k_list[i])\n",
    "        \n",
    "        if 5 in k_list[i]:\n",
    "            axs[i].axvline(x=5, color='orange', linestyle='--')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a7fbb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üßê Odd or Even?</b></h1>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><strong>1. Tie-Breaking:</strong> With an odd value of \\( k \\), ties in the voting process are less likely to occur. For example, with \\( k = 3 \\), there can't be a tie (assuming only two classes), while with \\( k = 4 \\), a tie can occur.</li>\n",
    "        <li><strong>2. Symmetry:</strong> An odd value of \\( k \\) ensures that the neighborhood is symmetric, meaning there is a clear distinction between the classes of the neighbors. This can lead to more stable and interpretable decisions.</li>\n",
    "        <li><strong>3. Robustness:</strong> Odd values of \\( k \\) tend to produce smoother decision boundaries, which can be more robust to noisy or irregular data compared to even values.</li>\n",
    "        <li><strong>4. Generalization:</strong> Odd values of \\( k \\) often lead to better generalization performance, as they are less sensitive to outliers and local fluctuations in the data.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be6b87",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üîç Explanation of Parameters \\( p = 1 \\) and \\( p = 2 \\)</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.3em;\">\n",
    "        <ul style=\"font-size:18px;line-height: 1.2em; line-height: 1.5em; font-family:calibri;\">\n",
    "            <li>In the K-nearest neighbors (KNN) algorithm, the parameter \\( p \\) determines the distance metric used for calculating the distance between data points.</li>\n",
    "            <li>When \\( p = 1 \\), it corresponds to the Manhattan distance , which calculates the distance as the sum of the absolute differences of the coordinates.</li>\n",
    "            <li>On the other hand, when \\( p = 2 \\), it corresponds to the Euclidean distance, which calculates the distance as the square root of the sum of the squared differences of the coordinates.</li>\n",
    "            <li>The choice between \\( p = 1 \\) and \\( p = 2 \\) affects how distances between data points are calculated and, consequently, how neighbors are identified in the KNN algorithm.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a261be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_evaluation_2x2([train_accuracies_p1, train_accuracies_p2], \n",
    "                         [test_accuracies_p1, test_accuracies_p2], \n",
    "                         [k_p1, k_p2], \n",
    "                         [\"\", \"\"],\n",
    "                         [1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd75dc",
   "metadata": {},
   "source": [
    "<a id=\"knn-evaluation\"></a>\n",
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.1.2 | KNN Evaluation</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_cross_validation(X, y, k=5, p=1, random_state=31, test_size=0.3, cv=5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, p=p)\n",
    "    cv_scores = cross_val_score(knn, X_train, y_train, cv=cv)\n",
    "    return cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracy_p1 = knn_cross_validation(X_knn, y_knn, k=5, p=1, random_state=31, test_size=0.3, cv=5)\n",
    "print(\"Cross Validation Accuracy:\", cv_accuracy_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracy_p2 = knn_cross_validation(X_knn, y_knn, k=5, p=2, random_state=31, test_size=0.3, cv=5)\n",
    "print(\"Cross Validation Accuracy:\", cv_accuracy_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdebde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_p1 = KNeighborsClassifier(n_neighbors=5, p=1)\n",
    "knn_p2 = KNeighborsClassifier(n_neighbors=5, p=2)\n",
    "\n",
    "knn_p1.fit(X_knn_train, y_knn_train)\n",
    "knn_p2.fit(X_knn_train, y_knn_train)\n",
    "\n",
    "train_predictions_p1 = knn_p1.predict(X_knn_train)\n",
    "test_predictions_p1 = knn_p1.predict(X_knn_test)\n",
    "\n",
    "train_predictions_p2 = knn_p2.predict(X_knn_train)\n",
    "test_predictions_p2 = knn_p2.predict(X_knn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfa5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(y_true_train, y_pred_train_p1, y_true_test, y_pred_test_p1, y_pred_train_p2, y_pred_test_p2):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "    cm_train_p1 = confusion_matrix(y_true_train, y_pred_train_p1)\n",
    "    sns.heatmap(cm_train_p1, annot=True, cmap='Purples', fmt='d', ax=axes[0, 0])\n",
    "    axes[0, 0].set_xlabel('Predicted')\n",
    "    axes[0, 0].set_ylabel('True')\n",
    "    axes[0, 0].set_title('Train Data (k=5) (p=1)')\n",
    "\n",
    "    cm_test_p1 = confusion_matrix(y_true_test, y_pred_test_p1)\n",
    "    sns.heatmap(cm_test_p1, annot=True, cmap='Purples', fmt='d', ax=axes[0, 1])\n",
    "    axes[0, 1].set_xlabel('Predicted')\n",
    "    axes[0, 1].set_ylabel('True')\n",
    "    axes[0, 1].set_title('Test Data (k=5) (p=1)')\n",
    "\n",
    "    cm_train_p2 = confusion_matrix(y_true_train, y_pred_train_p2)\n",
    "    sns.heatmap(cm_train_p2, annot=True, cmap='Purples', fmt='d', ax=axes[1, 0])\n",
    "    axes[1, 0].set_xlabel('Predicted')\n",
    "    axes[1, 0].set_ylabel('True')\n",
    "    axes[1, 0].set_title('Train Data (k=5) (p=2)')\n",
    "\n",
    "    cm_test_p2 = confusion_matrix(y_true_test, y_pred_test_p2)\n",
    "    sns.heatmap(cm_test_p2, annot=True, cmap='Purples', fmt='d', ax=axes[1, 1])\n",
    "    axes[1, 1].set_xlabel('Predicted')\n",
    "    axes[1, 1].set_ylabel('True')\n",
    "    axes[1, 1].set_title('Test Data (k=5) (p=2)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrices(y_knn_train, train_predictions_p1, y_knn_test, test_predictions_p1, train_predictions_p2, test_predictions_p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09973cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(k=5, p=1) (Train):\")\n",
    "print(classification_report(y_knn_train, train_predictions_p1))\n",
    "\n",
    "print(\"(k=5, p=1) (Test):\")\n",
    "print(classification_report(y_knn_test, test_predictions_p1))\n",
    "\n",
    "print(53*\"*\")\n",
    "\n",
    "print(\"(k=5, p=2) (Train):\")\n",
    "print(classification_report(y_knn_train, train_predictions_p2))\n",
    "\n",
    "print(\"(k=5, p=2) (Test):\")\n",
    "print(classification_report(y_knn_test, test_predictions_p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6c934",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "<h1 style=\"font-size:24px; font-family:calibri; color:#141140; text-align:center;\"><b>üåü Model Training & Test Comparison, k=5 & p=1 and k=5 & p=2</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fcce97",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "        <table style=\"font-size:20px; font-family:calibri; line-height: 1.5em; margin: 0 auto; width: 100%;\">\n",
    "            <tr>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Metric</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Training (p=1)</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Testing (p=1)</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Training (p=2)</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Testing (p=2)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Accuracy</td>\n",
    "                <td style=\"text-align:center;\">77%</td>\n",
    "                <td style=\"text-align:center;\">63%</td>\n",
    "                <td style=\"text-align:center;\">73%</td>\n",
    "                <td style=\"text-align:center;\">56%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Precision (class 0)</td>\n",
    "                <td style=\"text-align:center;\">85%</td>\n",
    "                <td style=\"text-align:center;\">86%</td>\n",
    "                <td style=\"text-align:center;\">69%</td>\n",
    "                <td style=\"text-align:center;\">67%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Precision (class 1)</td>\n",
    "                <td style=\"text-align:center;\">76%</td>\n",
    "                <td style=\"text-align:center;\">58%</td>\n",
    "                <td style=\"text-align:center;\">73%</td>\n",
    "                <td style=\"text-align:center;\">54%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Recall (class 0)</td>\n",
    "                <td style=\"text-align:center;\">34%</td>\n",
    "                <td style=\"text-align:center;\">29%</td>\n",
    "                <td style=\"text-align:center;\">28%</td>\n",
    "                <td style=\"text-align:center;\">19%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Recall (class 1)</td>\n",
    "                <td style=\"text-align:center;\">97%</td>\n",
    "                <td style=\"text-align:center;\">95%</td>\n",
    "                <td style=\"text-align:center;\">94%</td>\n",
    "                <td style=\"text-align:center;\">91%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">F1-score (class 0)</td>\n",
    "                <td style=\"text-align:center;\">49%</td>\n",
    "                <td style=\"text-align:center;\">43%</td>\n",
    "                <td style=\"text-align:center;\">40%</td>\n",
    "                <td style=\"text-align:center;\">30%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">F1-score (class 1)</td>\n",
    "                <td style=\"text-align:center;\">85%</td>\n",
    "                <td style=\"text-align:center;\">72%</td>\n",
    "                <td style=\"text-align:center;\">82%</td>\n",
    "                <td style=\"text-align:center;\">68%</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7ef09",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üéØ Conclusion of KNN Model Comparison (k=5)</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.5em;\">\n",
    "        <ul style=\"font-size:18px;line-height: 1.5em; font-family:calibri;\">\n",
    "            <li>The cross-validation accuracy for the KNN model with \\( p = 1 \\) (67.74%) is slightly higher than that for \\( p = 2 \\) (66.68%).</li>\n",
    "            <li>Although the difference in accuracy is small, it indicates that the \\( p = 1 \\) model tends to generalize slightly better than the \\( p = 2 \\) model.</li>\n",
    "            <li>This decision can be further supported by considering other factors such as precision, recall, and F1-score, where the \\( p = 1 \\) model consistently outperforms the \\( p = 2 \\) model.</li>\n",
    "            <li>Therefore, based on these cross-validation results and the overall performance metrics, it is advisable to choose the KNN model with \\( p = 1 \\) over the \\( p = 2 \\) model for classification tasks.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c26ad35",
   "metadata": {},
   "source": [
    "<a id=\"svm\"></a>\n",
    "# <p style=\"background-color: #A45EE5; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.2 | Support Vector Machine (SVM)</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e99b96",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">Robust Scaling Applied on SVM.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7124f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svm = df_scaled.drop([\"live\"], axis=1)\n",
    "y_svm = df_scaled[\"live\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca70e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svm_train, X_svm_test, y_svm_train, y_svm_test = train_test_split(X_svm, y_svm, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b26e3",
   "metadata": {},
   "source": [
    "<a id=\"svm-hpt\"></a>\n",
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.2.1 | SVM Hyper-parameter tuning</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d299ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_accuracy(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    This function plots the results of SVM with different values for C and kernels, \n",
    "    comparing the results between original and transformed data visually.\n",
    "    \"\"\"\n",
    "    C_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    kernels = ['linear', 'poly', 'rbf', 'sigmoid'] \n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8))  # 2x2 subplots\n",
    "\n",
    "    for i, kernel in enumerate(kernels):\n",
    "        ax = axs[i//2, i%2]\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        for C in C_values:\n",
    "            svm_clf = SVC(kernel=kernel, C=C)\n",
    "            svm_clf.fit(X_train, y_train)\n",
    "            train_scores.append(svm_clf.score(X_train, y_train))\n",
    "            test_scores.append(svm_clf.score(X_test, y_test))\n",
    "        ax.plot(C_values, train_scores, label='Training Accuracy', marker='o')\n",
    "        ax.plot(C_values, test_scores, label='Testing Accuracy', marker='x')\n",
    "        if kernel == 'poly':\n",
    "            ax.axvline(x=0.7, color='r', linestyle='--')\n",
    "        ax.set_title(f'SVM with {kernel.capitalize()} Kernel')\n",
    "        ax.set_xlabel('C')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        ax.set_xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "        ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da466581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_accuracy_degrees(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    This function plots the results of SVM with different values for C and kernels, \n",
    "    comparing the results between original and transformed data visually.\n",
    "    \"\"\"\n",
    "    C_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    poly_degrees = [1, 2, 3, 4]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8))  # 2x2 subplots\n",
    "\n",
    "    for i, degree in enumerate(poly_degrees):\n",
    "        ax = axs[i//2, i%2]\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        for C in C_values:\n",
    "            svm_clf = SVC(kernel='poly', degree=degree, C=C)\n",
    "            svm_clf.fit(X_train, y_train)\n",
    "            train_scores.append(svm_clf.score(X_train, y_train))\n",
    "            test_scores.append(svm_clf.score(X_test, y_test))\n",
    "        ax.plot(C_values, train_scores, label='Training Accuracy', marker='o')\n",
    "        ax.plot(C_values, test_scores, label='Testing Accuracy', marker='x')\n",
    "        if degree == 1:\n",
    "            ax.axvline(x=0.8, color='r', linestyle='--')\n",
    "        elif degree == 2:\n",
    "            ax.axvline(x=0.6, color='r', linestyle='--')\n",
    "        elif degree == 3:\n",
    "            ax.axvline(x=0.7, color='r', linestyle='--')\n",
    "        elif degree == 4:\n",
    "            ax.axvline(x=0.9, color='r', linestyle='--')\n",
    "        ax.set_title(f'SVM with Polynomial Kernel (Degree {degree})')\n",
    "        ax.set_xlabel('C')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        ax.set_xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "        ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3deb4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üîç Explanation of SVM Hyper-Parameters \\( C \\) and Kernels</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.3em;\">\n",
    "        <ul style=\"font-size:18px;line-height: 1.2em; line-height: 1.5em; font-family:calibri;\">\n",
    "            <li>The parameter \\( C \\) in Support Vector Machines (SVM) is the regularization parameter, which controls the trade-off between maximizing the margin and minimizing the classification error.</li>\n",
    "            <li>A smaller value of \\( C \\) leads to a softer margin, allowing more misclassifications but potentially better generalization to unseen data, while a larger value of \\( C \\) results in a harder margin, aiming to minimize training errors even at the expense of overfitting.</li>\n",
    "            <li>Choosing an appropriate value for \\( C \\) is crucial for balancing between bias and variance in SVM models. It is typically chosen through cross-validation.</li>\n",
    "            <li>Kernels in SVM define the decision function form by mapping the input data into a higher-dimensional feature space, allowing SVM to learn nonlinear decision boundaries.</li>\n",
    "            <li>The linear kernel \\( K(x, x') = x^Tx' \\) represents a linear decision boundary and is suitable for linearly separable or nearly separable data.</li>\n",
    "            <li>The polynomial kernel \\( K(x, x') = (x^Tx' + r)^d \\) maps the data into a higher-dimensional space using polynomial functions of degree \\( d \\) and is effective for capturing nonlinear relationships.</li>\n",
    "            <li>The radial basis function (RBF) kernel \\( K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2) \\) computes the similarity between data points based on their Euclidean distance in the input space and is versatile for handling various types of data distributions.</li>\n",
    "            <li>The sigmoid kernel \\( K(x, x') = \\tanh(\\alpha x^Tx' + c) \\) maps the data into a higher-dimensional space using hyperbolic tangent functions and is useful for neural network-like architectures.</li>\n",
    "            <li>Choosing the appropriate kernel depends on the problem at hand, the distribution of the data, and the computational complexity considerations.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm_accuracy(X_svm_train, X_svm_test, y_svm_train, y_svm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1e5c3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">Observing the superior performance of the polynomial kernel, particularly at \\( C = 0.7 \\), I opted to delve deeper into its efficacy across different polynomial degrees:</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm_accuracy_degrees(X_svm_train, X_svm_test, y_svm_train, y_svm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5e8ad",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">To optimize the performance of polynomial kernel SVMs, I carefully select a specific regularization parameter (\\( C \\)) for each degree. However, to ascertain which combination truly maximizes performance, cross-validation is applied to evaluate the chosen \\( C \\) for each degree rigorously.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a715ba9",
   "metadata": {},
   "source": [
    "<a id=\"svm-evaluation\"></a>\n",
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.2.2 | SVM Evaluation</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_cross_validation(X, y, kernel, degree, C, cv, test_size=0.3, random_state=31):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    svm_model = SVC(kernel=kernel, degree=degree, C=C)\n",
    "    scores = cross_val_score(svm_model, X_train, y_train, cv=cv)\n",
    "    \n",
    "    print(\"Cross-validation scores:\", scores)\n",
    "    print(\"Mean cross-validation score:\", scores.mean())\n",
    "    \n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cv_score_1 = svm_cross_validation(X_svm, y_svm, kernel='poly', degree=1, C=0.8, cv=5, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7496c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cv_score_2 = svm_cross_validation(X_svm, y_svm, kernel='poly', degree=2, C=0.6, cv=5, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e1b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cv_score_3 = svm_cross_validation(X_svm, y_svm, kernel='poly', degree=3, C=0.7, cv=5, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cv_score_4 = svm_cross_validation(X_svm, y_svm, kernel='poly', degree=4, C=0.9, cv=5, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c8c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_chart_svm():\n",
    "    degrees = [1, 2, 3, 4]\n",
    "    Cs = [0.8, 0.6, 0.7, 0.9]\n",
    "    mean_cv_scores = [0.6673684210526315, 0.6768421052631579, 0.7168421052631578, 0.7073684210526315]\n",
    "    colors = ['#6A0DAD', '#8E3A80', '#B656AF', '#D166B8']\n",
    "    labels = ['degree=1, C=0.8', 'degree=2, C=0.6', 'degree=3, C=0.7', 'degree=4, C=0.9']\n",
    "\n",
    "    sorted_data = sorted(zip(labels, mean_cv_scores), key=lambda x: x[1])\n",
    "    sorted_labels, sorted_mean_cv_scores = zip(*sorted_data)\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    bars = plt.barh(sorted_labels, sorted_mean_cv_scores, color=colors)\n",
    "\n",
    "    for bar, score in zip(bars, sorted_mean_cv_scores):\n",
    "        plt.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2, f'{score:.2f}', va='center', color='black')\n",
    "\n",
    "    plt.xlabel('Mean Cross-validation Score')\n",
    "    plt.ylabel('Parameters')\n",
    "    plt.title('Mean Cross-validation Score for Different Parameters on Polynomial Kernel')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_bar_chart_svm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5df276",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">Following thorough examination of various cross-validation results with different \\( C \\) values in the polynomial kernel, the optimal configuration emerged as degree=3 and \\( C \\)=0.7, yielding a cross-validation score of 72%. Therefore, let's proceed to train our SVM model using these hyperparameter values and evaluate the results:</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d598d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel='poly', degree=3, C=0.7)\n",
    "svm_model.fit(X_svm_train, y_svm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_svm = svm_model.predict(X_svm_train)\n",
    "test_predictions_svm = svm_model.predict(X_svm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(y_true_train, y_pred_train, y_true_test, y_pred_test):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "    cm_train = confusion_matrix(y_true_train, y_pred_train)\n",
    "    sns.heatmap(cm_train, annot=True, cmap='Purples', fmt='d', ax=axes[0])\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('True')\n",
    "    axes[0].set_title('Train Data (kernel=\"poly\", degree=3, C=0.7)')\n",
    "\n",
    "    cm_test = confusion_matrix(y_true_test, y_pred_test)\n",
    "    sns.heatmap(cm_test, annot=True, cmap='Purples', fmt='d', ax=axes[1])\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('True')\n",
    "    axes[1].set_title('Test Data (kernel=\"poly\", degree=3, C=0.7)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrices(y_svm_train, train_predictions_svm, y_svm_test, test_predictions_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Data (kernel=\"poly\", degree=3, C=0.7)')\n",
    "print(classification_report(y_svm_train, train_predictions_svm))\n",
    "\n",
    "print('Test Data (kernel=\"poly\", degree=3 ,C=0.7)')\n",
    "print(classification_report(y_svm_test, test_predictions_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c688364",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "<h1 style=\"font-size:24px; font-family:calibri; color:#141140; text-align:center;\"><b>üåü Model Training & Test Comparison, kernel='poly', degree=3, C=0.7 üåü</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225784b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <table style=\"font-size:20px; font-family:calibri; line-height: 1.5em; margin: 0 auto; width: 100%;\">\n",
    "        <tr>\n",
    "            <td style=\"font-weight:bold; text-align:center;\">Metric</td>\n",
    "            <td style=\"font-weight:bold; text-align:center;\">Training</td>\n",
    "            <td style=\"font-weight:bold; text-align:center;\">Testing</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">Accuracy</td>\n",
    "            <td style=\"text-align:center;\">79%</td>\n",
    "            <td style=\"text-align:center;\">65%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">Precision (class 0)</td>\n",
    "            <td style=\"text-align:center;\">100%</td>\n",
    "            <td style=\"text-align:center;\">100%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">Precision (class 1)</td>\n",
    "            <td style=\"text-align:center;\">76%</td>\n",
    "            <td style=\"text-align:center;\">59%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">Recall (class 0)</td>\n",
    "            <td style=\"text-align:center;\">34%</td>\n",
    "            <td style=\"text-align:center;\">29%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">Recall (class 1)</td>\n",
    "            <td style=\"text-align:center;\">100%</td>\n",
    "            <td style=\"text-align:center;\">100%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">F1-score (class 0)</td>\n",
    "            <td style=\"text-align:center;\">51%</td>\n",
    "            <td style=\"text-align:center;\">44%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">F1-score (class 1)</td>\n",
    "            <td style=\"text-align:center;\">86%</td>\n",
    "            <td style=\"text-align:center;\">75%</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb2e23",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üåü SVM Conclusion üåü</b></h1>\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">The SVM model with a polynomial kernel of degree 3 and \\( C = 0.7 \\) demonstrated strong performance in accurately classifying instances of class 1, achieving high precision and recall scores on both training and test data. However, its performance was less consistent for class 0, with lower recall scores indicating difficulty in correctly identifying instances of this class, particularly evident in the test data. While the model exhibited perfect precision for class 0, suggesting precision in its predictions, its lower recall scores suggest that it missed a notable portion of actual class 0 instances. This indicates a potential imbalance in the model's ability to detect instances across classes, emphasizing the need for further optimization to enhance its performance on class 0 instances.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7390d86",
   "metadata": {},
   "source": [
    "<a id=\"nb\"></a>\n",
    "# <p style=\"background-color: #A45EE5; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.3 | Naive Bayes (NB)</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5320cc2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>Explanation of Gaussian Naive Bayes:</b></h1>\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">Gaussian Naive Bayes is a variant of the Naive Bayes classifier that assumes the likelihood of the features to be Gaussian, i.e., normally distributed. It's well-suited for classification tasks where the features are continuous and follow a normal distribution. Each feature's distribution is estimated independently, and the class conditional probability is calculated using the Gaussian probability density function.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_df = df.drop([\"days\"], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8129472",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1351f1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>Why L2 Normalization for Gaussian Naive Bayes?</b></h1>\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">L2 normalization, also known as vector normalization, is a preprocessing technique applied to scale the features within the Gaussian Naive Bayes model. Gaussian Naive Bayes assumes that the features follow a Gaussian (normal) distribution. However, the scale of the features may vary, and features with larger magnitudes might dominate the prediction process, leading to biased results.</p>\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">By applying L2 normalization, we ensure that all feature vectors have a Euclidean norm of 1. This effectively scales down the features to the same magnitude, while preserving their direction. As a result, the Gaussian Naive Bayes model can make more accurate predictions by giving equal importance to all features, regardless of their original scale.</p>\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">Additionally, L2 normalization can help improve the robustness of the model by reducing the impact of outliers and extreme values in the dataset. This preprocessing step ensures that the model is less sensitive to variations in the data distribution, leading to more stable and reliable predictions.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalization(data):\n",
    "    norms = np.linalg.norm(data, axis=1, ord=2, keepdims=True)\n",
    "    normalized_data = data / norms\n",
    "    return normalized_data\n",
    "\n",
    "features_to_scale_nb = [\"dose_rate\", \"stage\", \"EQD2_HRCTV\", \"EQ_REC\", \"EQ_BLD\", \"EQ_SIG\", \"POINT_A_L\", \"POINT_A_R\", \"VOLUM\", \"TX_duration\", \"age\", \"smoke\", \"surgery\", \"marrage\", \"chemo\"]\n",
    "features_subset_nb = nb_df[features_to_scale_nb]\n",
    "\n",
    "scaled_features_nb_normalized = l2_normalization(features_subset_nb)\n",
    "\n",
    "nb_df[features_to_scale_nb] = scaled_features_nb_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35acfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nb = nb_df.drop([\"live\"], axis=1)\n",
    "y_nb = nb_df[\"live\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nb_train, X_nb_test, y_nb_train, y_nb_test = train_test_split(X_nb, y_nb, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae06f0",
   "metadata": {},
   "source": [
    "<a id=\"nb-hpt\"></a>\n",
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.3.1 | NB Hyper-parameter tuning</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac72120d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">No specify hyper parameter to tune this type of naive bayes.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06785fb9",
   "metadata": {},
   "source": [
    "<a id=\"nb-evaluation\"></a>\n",
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.3.2 | NB Evaluation</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8c1ec5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">With number 7 for cross validation, this algorithm performed better, (for other algorithm I applid number 5 for cross validation instead of naive bayes).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc04855",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "\n",
    "cv_scores = cross_val_score(gnb, X_nb_train, y_nb_train, cv=7)\n",
    "\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "\n",
    "print(mean_cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd3061",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; line-height: 1.5em; font-family:calibri;\">Cross Validation Score for alpha=10 is ~ 56%</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af409c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_nb_train, y_nb_train)\n",
    "\n",
    "y_train_pred_nb = gnb.predict(X_nb_train)\n",
    "y_test_pred_nb = gnb.predict(X_nb_test)\n",
    "\n",
    "cm_train = confusion_matrix(y_nb_train, y_train_pred_nb)\n",
    "cm_test = confusion_matrix(y_nb_test, y_test_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b781c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.heatmap(cm_train, annot=True, cmap='Purples', fmt='g', ax=ax[0])\n",
    "ax[0].set_title('Train Data Naive Bayes')\n",
    "ax[0].set_xlabel('Predicted labels')\n",
    "ax[0].set_ylabel('True labels')\n",
    "\n",
    "sns.heatmap(cm_test, annot=True, cmap='Purples', fmt='g', ax=ax[1])\n",
    "ax[1].set_title('Test Data Naive Bayes')\n",
    "ax[1].set_xlabel('Predicted labels')\n",
    "ax[1].set_ylabel('True labels')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(y_nb_train, y_train_pred_nb))\n",
    "\n",
    "print(53*\"*\")\n",
    "\n",
    "print(\"Test Data\")\n",
    "print(classification_report(y_nb_test, y_test_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be9295a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "<h1 style=\"font-size:24px; font-family:calibri; color:#141140; text-align:center;\"><b>üîç Model Evaluation Metrics for Gaussian Naive Bayes</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad108c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <table style=\"font-size:20px; font-family:calibri; line-height: 1.5em; margin: 0 auto; width: 100%;\">\n",
    "        <tr>\n",
    "            <td style=\"font-weight:bold; text-align:center;\">Metric</td>\n",
    "            <td style=\"font-weight:bold; text-align:center;\">Train Value</td>\n",
    "            <td style=\"font-weight:bold; text-align:center;\">Test Value</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">Accuracy</td>\n",
    "            <td style=\"text-align:center;\">69%</td>\n",
    "            <td style=\"text-align:center;\">60%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">Precision (class 0)</td>\n",
    "            <td style=\"text-align:center;\">51%</td>\n",
    "            <td style=\"text-align:center;\">60%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">Precision (class 1)</td>\n",
    "            <td style=\"text-align:center;\">82%</td>\n",
    "            <td style=\"text-align:center;\">61%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">Recall (class 0)</td>\n",
    "            <td style=\"text-align:center;\">69%</td>\n",
    "            <td style=\"text-align:center;\">57%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">Recall (class 1)</td>\n",
    "            <td style=\"text-align:center;\">69%</td>\n",
    "            <td style=\"text-align:center;\">64%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">F1-score (class 0)</td>\n",
    "            <td style=\"text-align:center;\">59%</td>\n",
    "            <td style=\"text-align:center;\">59%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center;\">F1-score (class 1)</td>\n",
    "            <td style=\"text-align:center;\">75%</td>\n",
    "            <td style=\"text-align:center;\">62%</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac95e86",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üåü Gaussian Naive Bayes Conclusion</b></h1>\n",
    "    <p style=\"font-size:23px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">The Gaussian Naive Bayes model demonstrated balanced performance across the training and testing datasets. While the accuracy on the training set was slightly higher compared to the testing set, the differences were relatively small. Notably, the precision, recall, and F1-score for class 1 were higher in the testing set compared to the training set, indicating potential overfitting on class 1 data. Conversely, the performance on class 0 metrics was more consistent between the training and testing sets. Overall, the model performed reasonably well on both datasets, particularly for class 1, where it demonstrated higher precision, recall, and F1-score values compared to class 0.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb7c57",
   "metadata": {},
   "source": [
    "<a id=\"dts\"></a>\n",
    "# <p style=\"background-color: #A45EE5; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.4 | Decision Trees (DTs)</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc18eb9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">\n",
    "Decision Trees (DT) and Random Forests (RF) do not require feature scaling because they make decisions based on splitting features at certain values rather than distances between data points. Scaling does not affect the tree structure or splitting criteria, and ensemble averaging in RF ensures that each tree's decisions are robust to feature scales. So I didn't apply feature scaling on them. Additionally, in ensemble methods like Random Forests, the final prediction is often determined by the majority vote of individual trees.\n",
    "</p >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d093e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dt = df.drop([\"live\", \"days\"], axis=1)\n",
    "y_dt = df[\"live\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dt_train, X_dt_test, y_dt_train, y_dt_test = train_test_split(X_dt, y_dt, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b96091",
   "metadata": {},
   "source": [
    "<a id=\"dts-hpt\"></a>\n",
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.4.1 | DTs Hyper-parameter tuning</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6624bcd3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üîç Explanation of Parameters \\( \\text{max_depth} \\) and \\( \\text{criterion} \\)</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.3em;\">\n",
    "        <ul style=\"font-size:18px;line-height: 1.2em; font-family:calibri;\">\n",
    "            <li>In Decision Trees, the parameter \\( \\text{max_depth} \\) determines the maximum depth of the tree, limiting the number of nodes and splits.</li>\n",
    "            <li>A shallow tree with a low max_depth may underfit the data by oversimplifying the relationships, while a deeper tree with a high max_depth may overfit by capturing noise in the training data.</li>\n",
    "            <li>The choice of \\( \\text{max_depth} \\) balances model complexity and performance, with optimal values often found through hyperparameter tuning.</li>\n",
    "            <li>The parameter \\( \\text{criterion} \\) specifies the function to measure the quality of a split.</li>\n",
    "            <li>Common criteria include 'gini' for Gini impurity and 'entropy' for information gain.</li>\n",
    "            <li>Gini impurity measures the frequency of misclassifications, while information gain measures the reduction in entropy (uncertainty).</li>\n",
    "            <li>The choice between 'gini' and 'entropy' impacts how the tree evaluates and selects the best splits during training.</li>\n",
    "            <li>Ultimately, the selection of \\( \\text{max_depth} \\) and \\( \\text{criterion} \\) influences the decision tree's structure and predictive performance.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2100c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dt_accuracy(X_train, y_train, X_test, y_test, x, cl, max_depth_range=None, random_state=31):\n",
    "    train_accuracies_gini_dt = []\n",
    "    test_accuracies_gini_dt = []\n",
    "    train_accuracies_entropy_dt = []\n",
    "    test_accuracies_entropy_dt = []\n",
    "\n",
    "    for max_depth in max_depth_range:\n",
    "        dt_gini = DecisionTreeClassifier(max_depth=max_depth, criterion='gini', random_state=random_state)\n",
    "        dt_entropy = DecisionTreeClassifier(max_depth=max_depth, criterion='entropy', random_state=random_state)\n",
    "\n",
    "        dt_gini.fit(X_train, y_train)\n",
    "        dt_entropy.fit(X_train, y_train)\n",
    "\n",
    "        train_accuracy_gini_dt = dt_gini.score(X_train, y_train)\n",
    "        test_accuracy_gini_dt = dt_gini.score(X_test, y_test)\n",
    "        train_accuracy_entropy_dt = dt_entropy.score(X_train, y_train)\n",
    "        test_accuracy_entropy_dt = dt_entropy.score(X_test, y_test)\n",
    "\n",
    "        train_accuracies_gini_dt.append(train_accuracy_gini_dt)\n",
    "        test_accuracies_gini_dt.append(test_accuracy_gini_dt)\n",
    "        train_accuracies_entropy_dt.append(train_accuracy_entropy_dt)\n",
    "        test_accuracies_entropy_dt.append(test_accuracy_entropy_dt)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.plot(max_depth_range, train_accuracies_gini_dt, color='purple', label='Train (Gini)')\n",
    "    plt.plot(max_depth_range, test_accuracies_gini_dt, color='purple', label='Test (Gini)', linestyle='--')\n",
    "\n",
    "    plt.plot(max_depth_range, train_accuracies_entropy_dt, color='blue', label='Train (Entropy)')\n",
    "    plt.plot(max_depth_range, test_accuracies_entropy_dt, color='blue', label='Test (Entropy)', linestyle='--')\n",
    "\n",
    "    plt.axvline(x=x, color=cl, linestyle='-')\n",
    "    \n",
    "    plt.title(f'Decision Trees with varying max_depth')\n",
    "    plt.xlabel('Max Depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xticks(max_depth_range) \n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dt_accuracy(X_dt_train, y_dt_train, X_dt_test, y_dt_test, x=4, cl='green', max_depth_range=range(1, 11), random_state=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c784bc7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140;\"></p>\n",
    "As we can see above, the decision tree performed better on max depth = 4 but let's examine the results on both entropy and gini.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fffc07",
   "metadata": {},
   "source": [
    "<a id=\"dts-evaluation\"></a>\n",
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.4.2 | DTs Evaluation</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df21c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=31)\n",
    "\n",
    "scores = cross_val_score(dt, X_dt, y_dt, cv=5)\n",
    "\n",
    "print(\"Average cross-validation score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666eb65",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140;\"></p>\n",
    "Cross validation for entropy is 61%.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ab094",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=31)\n",
    "\n",
    "scores = cross_val_score(dt, X_dt, y_dt, cv=5)\n",
    "\n",
    "print(\"Average cross-validation score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22586192",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140;\"></p>\n",
    "Cross validation for entropy is 54%.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=31)\n",
    "dt_entropy.fit(X_dt_train, y_dt_train)\n",
    "\n",
    "y_train_pred_entropy = dt_entropy.predict(X_dt_train)\n",
    "y_test_pred_entropy = dt_entropy.predict(X_dt_test)\n",
    "\n",
    "conf_matrix_train_entropy = confusion_matrix(y_dt_train, y_train_pred_entropy)\n",
    "conf_matrix_test_entropy = confusion_matrix(y_dt_test, y_test_pred_entropy)\n",
    "\n",
    "dt_gini = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=31)\n",
    "dt_gini.fit(X_dt_train, y_dt_train)\n",
    "\n",
    "y_train_pred_gini = dt_gini.predict(X_dt_train)\n",
    "y_test_pred_gini = dt_gini.predict(X_dt_test)\n",
    "\n",
    "conf_matrix_train_gini = confusion_matrix(y_dt_train, y_train_pred_gini)\n",
    "conf_matrix_test_gini = confusion_matrix(y_dt_test, y_test_pred_gini)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(conf_matrix_train_entropy, annot=True, cmap='Purples', fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(y_dt), yticklabels=np.unique(y_dt))\n",
    "plt.title('Train Set (criterion=\"entropy\", max_depth=4)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.heatmap(conf_matrix_test_entropy, annot=True, cmap='Purples', fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(y_dt), yticklabels=np.unique(y_dt))\n",
    "plt.title('Test Set (criterion=\"entropy\", max_depth=4)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.heatmap(conf_matrix_train_gini, annot=True, cmap='Purples', fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(y_dt), yticklabels=np.unique(y_dt))\n",
    "plt.title('Train Set (criterion=\"gini\", max_depth=4)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.heatmap(conf_matrix_test_gini, annot=True, cmap='Purples', fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(y_dt), yticklabels=np.unique(y_dt))\n",
    "plt.title('Test Set (criterion=\"gini\", max_depth=4)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86be788",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=31)\n",
    "decision_tree_entropy.fit(X_dt_train, y_dt_train)\n",
    "\n",
    "train_predictions_entropy = decision_tree_entropy.predict(X_dt_train)\n",
    "test_predictions_entropy = decision_tree_entropy.predict(X_dt_test)\n",
    "\n",
    "print(\"Training Data - Entropy Criterion:\")\n",
    "print(classification_report(y_dt_train, train_predictions_entropy))\n",
    "print(53*\"-\")\n",
    "print(\"Testing Data - Entropy Criterion:\")\n",
    "print(classification_report(y_dt_test, test_predictions_entropy))\n",
    "\n",
    "decision_tree_gini = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=31)\n",
    "decision_tree_gini.fit(X_dt_train, y_dt_train)\n",
    "print(53*\"*\")\n",
    "train_predictions_gini = decision_tree_gini.predict(X_dt_train)\n",
    "test_predictions_gini = decision_tree_gini.predict(X_dt_test)\n",
    "\n",
    "print(\"Training Data - Gini Criterion:\")\n",
    "print(classification_report(y_dt_train, train_predictions_gini))\n",
    "print(53*\"-\")\n",
    "print(\"Testing Data - Gini Criterion:\")\n",
    "print(classification_report(y_dt_test, test_predictions_gini))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c3952",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "<h1 style=\"font-size:24px; font-family:calibri; color:#141140; text-align:center;\"><b>üåü Model Training & Test Comparison, max_depth=4 & criterion=\"entropy\" and \"gini\"</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58b469c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "        <table style=\"font-size:20px; font-family:calibri; line-height: 1.5em; margin: 0 auto; width: 100%;\">\n",
    "            <tr>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Metric</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Training (Gini)</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Testing (Gini)</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Training (Entropy)</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Testing (Entropy)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Accuracy</td>\n",
    "                <td style=\"text-align:center;\">82%</td>\n",
    "                <td style=\"text-align:center;\">63%</td>\n",
    "                <td style=\"text-align:center;\">76%</td>\n",
    "                <td style=\"text-align:center;\">60%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Precision (class 0)</td>\n",
    "                <td style=\"text-align:center;\">100%</td>\n",
    "                <td style=\"text-align:center;\">86%</td>\n",
    "                <td style=\"text-align:center;\">67%</td>\n",
    "                <td style=\"text-align:center;\">62%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Precision (class 1)</td>\n",
    "                <td style=\"text-align:center;\">79%</td>\n",
    "                <td style=\"text-align:center;\">58%</td>\n",
    "                <td style=\"text-align:center;\">79%</td>\n",
    "                <td style=\"text-align:center;\">59%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Recall (class 0)</td>\n",
    "                <td style=\"text-align:center;\">44%</td>\n",
    "                <td style=\"text-align:center;\">29%</td>\n",
    "                <td style=\"text-align:center;\">50%</td>\n",
    "                <td style=\"text-align:center;\">48%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Recall (class 1)</td>\n",
    "                <td style=\"text-align:center;\">100%</td>\n",
    "                <td style=\"text-align:center;\">95%</td>\n",
    "                <td style=\"text-align:center;\">88%</td>\n",
    "                <td style=\"text-align:center;\">73%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">F1-score (class 0)</td>\n",
    "                <td style=\"text-align:center;\">61%</td>\n",
    "                <td style=\"text-align:center;\">43%</td>\n",
    "                <td style=\"text-align:center;\">57%</td>\n",
    "                <td style=\"text-align:center;\">54%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">F1-score (class 1)</td>\n",
    "                <td style=\"text-align:center;\">88%</td>\n",
    "                <td style=\"text-align:center;\">72%</td>\n",
    "                <td style=\"text-align:center;\">83%</td>\n",
    "                <td style=\"text-align:center;\">65%</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a76f47",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140;\">Results of max_depth=4 and criterion=\"entropy\" is more balanced in different metrics and also it had higher cross validation , So I decided to choose it.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33e5e4",
   "metadata": {},
   "source": [
    "<a id=\"rf\"></a>\n",
    "# <p style=\"background-color: #A45EE5; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.5 | Random Forest (RF)</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbcb83e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üåü Explanation of Random Forest Challenges and Strengths üåü</b></h1>\n",
    "    <p style=\"font-size:23px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">\n",
    "        <ol style=\"font-size:20px;line-height: 1.5em; font-family:calibri;\">\n",
    "            <li><b>Challenges:</b>\n",
    "                <ul>\n",
    "                    <li>Overfitting: Random Forests can be prone to overfitting, especially if the trees are allowed to grow very deep. Regularization techniques such as limiting the depth of trees or pruning can mitigate this issue.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li><b>Strengths:</b>\n",
    "                <ul>\n",
    "                    <li>Robustness to Noise: Random Forests are robust to noise and outliers in the data due to the averaging effect of multiple trees.</li>\n",
    "                    <li>Feature Importance: Random Forests provide a feature importance measure, allowing users to understand which features are most influential in making predictions.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ol>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf = df.drop([\"live\", \"days\"], axis=1)\n",
    "y_rf = df[\"live\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf_train, X_rf_test, y_rf_train, y_rf_test = train_test_split(X_rf, y_rf, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3dfdff",
   "metadata": {},
   "source": [
    "<a id=\"rf-hpt\"></a>\n",
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.5.1 | RF Hyper-parameter tuning</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28926ea3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üåü Explanation of Random Forest Hyperparameters üåü</b></h1>\n",
    "    <p style=\"font-size:23px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">\n",
    "        <ol style=\"font-size:20px;line-height: 1.5em; font-family:calibri;\">\n",
    "            <li><b>n_estimators:</b> Number of trees in the forest. Increasing this parameter generally improves the performance of the model, but it also increases computation time.</li>\n",
    "            <li><b>criterion:</b> The function used to measure the quality of a split. 'gini' refers to the Gini impurity, while 'entropy' refers to information gain. These criteria are used to decide the best split at each node.</li>\n",
    "            <li><b>max_depth:</b> The maximum depth of the tree. Increasing this parameter allows the tree to grow deeper, capturing more information about the data. However, deeper trees may lead to overfitting.</li>\n",
    "        </ol>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b10b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rf_accuracy(X_train, y_train, X_test, y_test, x, cl,  max_depth=None, n_estimators_range=np.arange(1, 22), random_state=31, ):\n",
    "    train_accuracies_gini_rf = []\n",
    "    test_accuracies_gini_rf = []\n",
    "    train_accuracies_entropy_rf = []\n",
    "    test_accuracies_entropy_rf = []\n",
    "\n",
    "    for n_estimators in n_estimators_range:\n",
    "        rf_gini = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, criterion='gini', random_state=random_state)\n",
    "        rf_entropy = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, criterion='entropy', random_state=random_state)\n",
    "\n",
    "        rf_gini.fit(X_train, y_train)\n",
    "        rf_entropy.fit(X_train, y_train)\n",
    "\n",
    "        train_accuracy_gini_rf = rf_gini.score(X_train, y_train)\n",
    "        test_accuracy_gini_rf = rf_gini.score(X_test, y_test)\n",
    "        train_accuracy_entropy_rf = rf_entropy.score(X_train, y_train)\n",
    "        test_accuracy_entropy_rf = rf_entropy.score(X_test, y_test)\n",
    "\n",
    "        train_accuracies_gini_rf.append(train_accuracy_gini_rf)\n",
    "        test_accuracies_gini_rf.append(test_accuracy_gini_rf)\n",
    "        train_accuracies_entropy_rf.append(train_accuracy_entropy_rf)\n",
    "        test_accuracies_entropy_rf.append(test_accuracy_entropy_rf)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(n_estimators_range, train_accuracies_gini_rf, color='purple', label='Train (Gini)')\n",
    "    plt.plot(n_estimators_range, test_accuracies_gini_rf, color='purple', label='Test (Gini)', linestyle='--')\n",
    "    plt.plot(n_estimators_range, train_accuracies_entropy_rf, color='red', label='Train (Entropy)')\n",
    "    plt.plot(n_estimators_range, test_accuracies_entropy_rf, color='red', label='Test (Entropy)', linestyle='--')\n",
    "    \n",
    "    plt.axvline(x=x, color=cl, linestyle='-')\n",
    "    \n",
    "    plt.title(f'Random Forest with max_depth={max_depth}')\n",
    "    plt.xlabel('Number of Estimators')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xticks(n_estimators_range) \n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rf_accuracy(X_rf_train, y_rf_train, X_rf_test, y_rf_test, x=12, cl=\"darkblue\", max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b52fc1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:16px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">\n",
    "The blue line points to the best performance, Let's build models on it:</p >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea6e2e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "<h1 style=\"font-size:24px; font-family:calibri; color:#141140; text-align:center;\"><b>üåü Build Simple Random Forest Model  üåü</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd67e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_classifier_gini = RandomForestClassifier(n_estimators=12, criterion='gini', max_depth=5, random_state=31)\n",
    "rf_classifier_gini.fit(X_rf_train, y_rf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_ent = RandomForestClassifier(n_estimators=12, criterion='entropy', max_depth=5, random_state=31)\n",
    "rf_classifier_ent.fit(X_rf_train, y_rf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c984055a",
   "metadata": {},
   "source": [
    "<a id=\"rf-evaluation\"></a>\n",
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.5.2 | RF Evaluation</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3934338",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:16px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">\n",
    "Evaluation of simple model on  {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 12}:</p >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ad044",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_gini = cross_val_score(rf_classifier_gini, X_rf_train, y_rf_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-validation Scores:\", cv_scores_gini)\n",
    "cv_mean_score_gini = cv_scores_gini.mean()\n",
    "print(\"Mean Cross-validation Score:\", cv_mean_score_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da569d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:16px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">\n",
    "Average test accuracy on gini : ~ 65%</p >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe00195",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6CC; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:16px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">\n",
    "</p >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f0889",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:16px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">\n",
    "Evaluation of simple model on  {'criterion': 'entropy', 'max_depth': 5, 'n_estimators': 12}:</p >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434138f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_ent = cross_val_score(rf_classifier_ent, X_rf_train, y_rf_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-validation Scores:\", cv_scores_ent)\n",
    "cv_mean_score_ent = cv_scores_ent.mean()\n",
    "print(\"Mean Cross-validation Score:\", cv_mean_score_ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2c615",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:16px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">\n",
    "Average test accuracy on entropy : ~ 60%</p >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ebdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gini_train = {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 12}\n",
    "params_entropy_train = {'criterion': 'entropy', 'max_depth': 5, 'n_estimators': 12}\n",
    "\n",
    "rf_classifier_gini_train = RandomForestClassifier(**params_gini_train)\n",
    "rf_classifier_entropy_train = RandomForestClassifier(**params_entropy_train)\n",
    "\n",
    "rf_classifier_gini_train.fit(X_rf_train, y_rf_train)\n",
    "rf_classifier_entropy_train.fit(X_rf_train, y_rf_train)\n",
    "\n",
    "y_rf_train_pred_gini = rf_classifier_gini_train.predict(X_rf_train)\n",
    "y_rf_test_pred_gini = rf_classifier_gini_train.predict(X_rf_test)\n",
    "\n",
    "y_rf_train_pred_entropy = rf_classifier_entropy_train.predict(X_rf_train)\n",
    "y_rf_test_pred_entropy = rf_classifier_entropy_train.predict(X_rf_test)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(confusion_matrix(y_rf_train, y_rf_train_pred_gini), annot=True, cmap='Purples', fmt='d')\n",
    "plt.title(\"Gini, max_depth=5, n_estimators=12, Train\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.heatmap(confusion_matrix(y_rf_test, y_rf_test_pred_gini), annot=True, cmap='Purples', fmt='d')\n",
    "plt.title('Gini, max_depth=5, n_estimators=12, Test')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.heatmap(confusion_matrix(y_rf_train, y_rf_train_pred_entropy), annot=True, cmap='Purples', fmt='d')\n",
    "plt.title(\"Entropy, max_depth=5, n_estimators=12, Train\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.heatmap(confusion_matrix(y_rf_test, y_rf_test_pred_entropy), annot=True, cmap='Purples', fmt='d')\n",
    "plt.title('Entropy, max_depth=5, n_estimators=12, Test')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aebdc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rf_train_pred_ent = rf_classifier_ent.predict(X_rf_train)\n",
    "\n",
    "print(\"Training Classification Report for Entropy Criterion:\")\n",
    "print(classification_report(y_rf_train, y_rf_train_pred_ent))\n",
    "\n",
    "y_rf_test_pred_ent = rf_classifier_ent.predict(X_rf_test)\n",
    "print(\"\\nTesting Classification Report for Entropy Criterion:\")\n",
    "print(classification_report(y_rf_test, y_rf_test_pred_ent))\n",
    "\n",
    "y_rf_train_pred_gini = rf_classifier_gini.predict(X_rf_train)\n",
    "print(\"\\nTraining Classification Gini Report:\")\n",
    "print(classification_report(y_rf_train, y_rf_train_pred_gini))\n",
    "\n",
    "y_rf_test_pred_gini = rf_classifier_gini.predict(X_rf_test)\n",
    "\n",
    "print(\"\\nTesting Classification Gini Report:\")\n",
    "print(classification_report(y_rf_test, y_rf_test_pred_gini))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e828e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "<h1 style=\"font-size:24px; font-family:calibri; color:#141140; text-align:center;\"><b>üåü Model Comparison, Gini & Entropy</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa04106",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "        <table style=\"font-size:20px; font-family:calibri; line-height: 1.5em; margin: 0 auto; width: 100%;\">\n",
    "            <tr>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Metric</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Training (Gini)</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Testing (Gini)</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Training (Entropy)</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Testing (Entropy)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Accuracy</td>\n",
    "                <td style=\"text-align:center;\">84%</td>\n",
    "                <td style=\"text-align:center;\">70%</td>\n",
    "                <td style=\"text-align:center;\">80%</td>\n",
    "                <td style=\"text-align:center;\">53%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Precision (class 0)</td>\n",
    "                <td style=\"text-align:center;\">94%</td>\n",
    "                <td style=\"text-align:center;\">83%</td>\n",
    "                <td style=\"text-align:center;\">93%</td>\n",
    "                <td style=\"text-align:center;\">57%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Precision (class 1)</td>\n",
    "                <td style=\"text-align:center;\">81%</td>\n",
    "                <td style=\"text-align:center;\">65%</td>\n",
    "                <td style=\"text-align:center;\">78%</td>\n",
    "                <td style=\"text-align:center;\">53%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Recall (class 0)</td>\n",
    "                <td style=\"text-align:center;\">53%</td>\n",
    "                <td style=\"text-align:center;\">48%</td>\n",
    "                <td style=\"text-align:center;\">41%</td>\n",
    "                <td style=\"text-align:center;\">19%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Recall (class 1)</td>\n",
    "                <td style=\"text-align:center;\">99%</td>\n",
    "                <td style=\"text-align:center;\">91%</td>\n",
    "                <td style=\"text-align:center;\">99%</td>\n",
    "                <td style=\"text-align:center;\">86%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">F1-score (class 0)</td>\n",
    "                <td style=\"text-align:center;\">68%</td>\n",
    "                <td style=\"text-align:center;\">61%</td>\n",
    "                <td style=\"text-align:center;\">57%</td>\n",
    "                <td style=\"text-align:center;\">29%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">F1-score (class 1)</td>\n",
    "                <td style=\"text-align:center;\">89%</td>\n",
    "                <td style=\"text-align:center;\">75%</td>\n",
    "                <td style=\"text-align:center;\">87%</td>\n",
    "                <td style=\"text-align:center;\">66%</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b9fd5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üåü Random Forest Conclusion</b></h1>\n",
    "    <p style=\"font-size:23px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">The random forest model, initially trained with Gini criterion, achieved 84% training and 70% testing accuracy. After tuning with Entropy criterion, training accuracy improved to 80% while testing accuracy dropped to 53%. Precision, recall, and F1-score metrics showed mixed changes, indicating the importance of criterion choice in model performance. Also it has more accuracy on cross validation for gini. So my choice is criterion=\"gini\", max_depth=5 and n_estimators=12.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d889ff5",
   "metadata": {},
   "source": [
    "<a id=\"mlp\"></a>\n",
    "# <p style=\"background-color: #A45EE5; font-family:calibri; color:white; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.6 | Multilayer Perceptron (MLP)</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be9b72",
   "metadata": {},
   "source": [
    "<a id=\"mlp-hpt\"></a>\n",
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.6.1 | MLP Hyper-parameter tuning</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53277275",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üåü Explanation of MLP Hyperparameters üåü</b></h1>\n",
    "    <p style=\"font-size:23px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">\n",
    "        <ol style=\"font-size:20px;line-height: 1.5em; font-family:calibri;\">\n",
    "            <li><b>hidden_layer_sizes:</b> This parameter determines the architecture of the neural network by specifying the number of neurons in each hidden layer. For example, `hidden_layer_sizes=(100,)` indicates a single hidden layer with 100 neurons. Multiple values can be provided to specify multiple hidden layers.</li>\n",
    "            <li><b>max_iter:</b> The maximum number of iterations (epochs) the training algorithm will run to optimize the neural network's weights. It controls how many times the entire training dataset is passed forward and backward through the network during training.</li>\n",
    "            <li><b>alpha:</b> This regularization parameter, also known as the L2 penalty term, helps mitigate overfitting by adding a penalty to the loss function during training. It adjusts the regularization strength, with higher values of alpha leading to stronger regularization. This regularization discourages overly complex models, promoting generalization to unseen data.</li>\n",
    "        </ol>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6261d0f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üîç Sensitivity of MLP to Outliers and Use of Robust Scaler</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.3em;\">\n",
    "        <ul style=\"font-size:20px;line-height: 1.5em; font-family:calibri;\">\n",
    "            <li>Multi-Layer Perceptrons (MLPs) can be sensitive to outliers in the input data.</li>\n",
    "            <li>Outliers can disproportionately influence the training process, affecting the model's weights and biases, and potentially leading to poor generalization performance.</li>\n",
    "            <li>Robust Scaler is a preprocessing technique used to mitigate the impact of outliers on numerical data.</li>\n",
    "            <li>Robust Scaler scales features using statistics that are robust to outliers, such as the median and interquartile range (IQR).</li>\n",
    "            <li>By scaling the features using robust statistics, Robust Scaler reduces the influence of outliers, making the training process more robust and improving the stability and performance of the MLP model.</li>\n",
    "            <li>Thus, using Robust Scaler for outliers in numerical data helps to improve the overall performance and reliability of MLP models, especially in scenarios where outliers are present in the dataset.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies_mlp = []\n",
    "test_accuracies_mlp = []\n",
    "\n",
    "alpha_values = np.arange(0.1, 2.6, 0.1)\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=100, alpha=alpha, random_state=31)\n",
    "\n",
    "    mlp.fit(X_knn_train, y_knn_train)\n",
    "    \n",
    "    train_accuracy_mlp = mlp.score(X_knn_train, y_knn_train)\n",
    "    train_accuracies_mlp.append(train_accuracy_mlp)\n",
    "    \n",
    "    test_accuracy_mlp = mlp.score(X_knn_test, y_knn_test)\n",
    "    test_accuracies_mlp.append(test_accuracy_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))  \n",
    "plt.plot(alpha_values, train_accuracies_mlp, label='Train Accuracy')\n",
    "plt.plot(alpha_values, test_accuracies_mlp, label='Test Accuracy')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--') \n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Impact of Alpha on Train and Test Accuracies')\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(0.1, 2.6, 0.1), rotation=65)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f12b6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">I decided to choose alpha=0.5 to train MLP algorithm on it.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472a647",
   "metadata": {},
   "source": [
    "<a id=\"mlp-evaluation\"></a>\n",
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Step 10.6.2 | MLP Evaluation</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f84da6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">Train MLP on (hidden_layer_sizes=(100,), max_iter=150, alpha=0.5, random_state=31) that founded from hyper parameter tuning.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_value = 0.5\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=150, alpha=alpha_value, random_state=31)\n",
    "\n",
    "mlp.fit(X_knn_train, y_knn_train)\n",
    "\n",
    "train_predictions = mlp.predict(X_knn_train)\n",
    "test_predictions = mlp.predict(X_knn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e481a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "train_conf_matrix = confusion_matrix(y_knn_train, train_predictions)\n",
    "sns.heatmap(train_conf_matrix, annot=True, cmap='Purples', fmt='d', cbar=False)\n",
    "plt.title('Training Data (Alpha=2.1)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "test_conf_matrix = confusion_matrix(y_knn_test, test_predictions)\n",
    "sns.heatmap(test_conf_matrix, annot=True, cmap='Purples', fmt='d', cbar=False)\n",
    "plt.title('Testing Data (Alpha=2.1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c6805",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=150, alpha=alpha_value, random_state=31)\n",
    "\n",
    "cv_scores_mlp = cross_val_score(mlp, X_knn, y_knn, cv=5)\n",
    "\n",
    "print(\"Cross-Validation Scores:\")\n",
    "print(cv_scores_mlp)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929aafb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">Cross validation score for MLP on (hidden_layer_sizes=(100,), max_iter=150, alpha=0.5, random_state=31) is ~ 63.5%.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a956e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data (Alpha=0.5)\")\n",
    "print(classification_report(y_knn_train, train_predictions))\n",
    "\n",
    "print(53*\"*\")\n",
    "\n",
    "print(\"Testing Data: (Alpha=0.5)\")\n",
    "print(classification_report(y_knn_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83550f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "<h1 style=\"font-size:24px; font-family:calibri; color:#141140; text-align:center;\"><b>üåü MLP Results on Tran & Test Accuracies on Alpha=0.5</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9930b05",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "        <table style=\"font-size:20px; font-family:calibri; line-height: 1.5em; margin: 0 auto; width: 100%;\">\n",
    "            <tr>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Metric</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Training</td>\n",
    "                <td style=\"font-weight:bold; text-align:center;\">Testing</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Accuracy</td>\n",
    "                <td style=\"text-align:center;\">88%</td>\n",
    "                <td style=\"text-align:center;\">65%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Precision (class 0)</td>\n",
    "                <td style=\"text-align:center;\">88%</td>\n",
    "                <td style=\"text-align:center;\">75%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Precision (class 1)</td>\n",
    "                <td style=\"text-align:center;\">88%</td>\n",
    "                <td style=\"text-align:center;\">61%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Recall (class 0)</td>\n",
    "                <td style=\"text-align:center;\">72%</td>\n",
    "                <td style=\"text-align:center;\">43%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">Recall (class 1)</td>\n",
    "                <td style=\"text-align:center;\">96%</td>\n",
    "                <td style=\"text-align:center;\">86%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">F1-score (class 0)</td>\n",
    "                <td style=\"text-align:center;\">79%</td>\n",
    "                <td style=\"text-align:center;\">55%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center;\">F1-score (class 1)</td>\n",
    "                <td style=\"text-align:center;\">91%</td>\n",
    "                <td style=\"text-align:center;\">72%</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be3082",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üåü MLP Conclusion üåü</b></h1>\n",
    "    <p style=\"font-size:23px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.2em;\">The MLP model achieved an accuracy of 88% on the training data and 65% on the testing data. In terms of precision, there was a slight decrease from training to testing for both class 0 (88% to 75%) and class 1 (88% to 61%), indicating a potential issue with false positive rates in the testing set. The recall for class 0 decreased from training to testing (72% to 43%), suggesting that the model missed more true negative instances in the testing set. However, the recall for class 1 remained high and consistent between training and testing (96% to 86%), indicating that the model effectively captured most of the positive instances. The F1-score for both classes showed a decrease from training to testing (class 0: 79% to 55%, class 1: 91% to 72%), indicating a decline in overall model performance on the testing data compared to the training data.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332bf2a",
   "metadata": {},
   "source": [
    "<a id=\"survival\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 11 | C-Index, ROC-AUC</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903aef9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üåü Explanation of ROC Curve, AUC, and C-index üåü</b></h1>\n",
    "    <ol style=\"font-size:20px;line-height: 1.5em; font-family:calibri; padding-left: 20px;\">\n",
    "        <li>\n",
    "            <b>What is the ROC curve?</b><br>\n",
    "            The ROC curve, or Receiver Operating Characteristic curve, is a graphical representation of the effectiveness of a binary classification model. It illustrates the trade-off between the true positive rate (TPR) and the false positive rate (FPR) across different classification thresholds. A higher TPR and lower FPR indicate better performance of the model.\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>What does AUC measure?</b><br>\n",
    "            AUC, or Area Under the ROC Curve, quantifies the overall performance of a binary classification model. It represents the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative instance. AUC ranges from 0 to 1, where 0.5 indicates no discrimination (random guessing) and values closer to 1 indicate better discrimination.\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>Is C-index the same as AUC?</b><br>\n",
    "            In binary classification tasks, both the AUC and the C-index, also known as the concordance index, are equivalent measures of predictive performance. They assess the model's ability to correctly rank orderings of pairs of instances with respect to their predicted probabilities or scores. Therefore, AUC = C-index holds true for binary classification tasks, indicating that both metrics provide the same evaluation of the model's predictive performance in terms of discrimination.\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>What is a good AUC score?</b><br>\n",
    "            AUC scores between 0.7 to 0.8 are considered acceptable, 0.8 to 0.9 are excellent, and values above 0.9 are outstanding. AUC values closer to 0.5 suggest poor discrimination, while values closer to 1 indicate better performance in distinguishing between positive and negative instances.\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>What is a good C-index?</b><br>\n",
    "            Similarly to the AUC, C-index=1 corresponds to the best model prediction, and C-index=0.5 represents a random prediction. In binary classification tasks, both the AUC and the C-index provide a reliable assessment of the model's discriminatory power, indicating how well it can distinguish between positive and negative instances.\n",
    "        </li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7acc9b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üåü Evaluating Classification Algorithms with ROC-AUC üåü</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.5em;\">In order to determine the best performing classification algorithm among the six we've examined, we'll apply the Receiver Operating Characteristic (ROC) curve and Area Under the Curve (AUC) metric. As our task involves binary classification, it's important to note that the C-Index, equivalent to AUC, will be our guiding metric. This evaluation will help us identify the model that exhibits the highest discriminatory power in distinguishing between positive and negative instances.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, p=1)\n",
    "knn_classifier.fit(X_knn_train, y_knn_train)\n",
    "\n",
    "svm_classifier = SVC(kernel='poly', C=0.7, degree=3, probability=True)\n",
    "svm_classifier.fit(X_svm_train, y_svm_train)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(criterion='gini', max_depth=5, n_estimators=12, random_state=31)\n",
    "rf_classifier.fit(X_rf_train, y_rf_train)\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=4, criterion='entropy', random_state=31)\n",
    "dt_classifier.fit(X_dt_train, y_dt_train)\n",
    "\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_nb_train, y_nb_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=150, alpha=0.5, random_state=31)\n",
    "mlp_classifier.fit(X_knn_train, y_knn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8cd30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred_proba = knn_classifier.predict_proba(X_knn_test)[:, 1]\n",
    "\n",
    "svm_pred_proba = svm_classifier.predict_proba(X_svm_test)[:, 1]\n",
    "\n",
    "rf_pred_proba = rf_classifier.predict_proba(X_rf_test)[:, 1]\n",
    "\n",
    "dt_pred_proba = dt_classifier.predict_proba(X_dt_test)[:, 1]\n",
    "\n",
    "nb_pred_proba = nb_classifier.predict_proba(X_nb_test)[:, 1]\n",
    "\n",
    "mlp_pred_proba = mlp_classifier.predict_proba(X_knn_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_knn, tpr_knn, _ = roc_curve(y_knn_test, knn_pred_proba)\n",
    "roc_auc_knn = auc(fpr_knn, tpr_knn)\n",
    "\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_svm_test, svm_pred_proba)\n",
    "roc_auc_svm = auc(fpr_svm, tpr_svm)\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_rf_test, rf_pred_proba)\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_dt_test, dt_pred_proba)\n",
    "roc_auc_dt = auc(fpr_dt, tpr_dt)\n",
    "\n",
    "fpr_nb, tpr_nb, _ = roc_curve(y_nb_test, nb_pred_proba)\n",
    "roc_auc_nb = auc(fpr_nb, tpr_nb)\n",
    "\n",
    "fpr_mlp, tpr_mlp, _ = roc_curve(y_nb_test, mlp_pred_proba)\n",
    "roc_auc_mlp = auc(fpr_mlp, tpr_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].plot(fpr_knn, tpr_knn, color='blue', lw=2, label='KNN (AUC = %0.2f)' % roc_auc_knn)\n",
    "axes[0, 0].fill_between(fpr_knn, tpr_knn, color='blue', alpha=0.3)\n",
    "axes[0, 0].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "axes[0, 0].set_xlim([0.0, 1.0])\n",
    "axes[0, 0].set_ylim([0.0, 1.05])\n",
    "axes[0, 0].set_xlabel('False Positive Rate (FPR)')\n",
    "axes[0, 0].set_ylabel('True Positive Rate (TPR)')\n",
    "axes[0, 0].set_title('KNN ROC Curve')\n",
    "axes[0, 0].legend(loc='lower right')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(fpr_svm, tpr_svm, color='red', lw=2, label='SVM (AUC = %0.2f)' % roc_auc_svm)\n",
    "axes[0, 1].fill_between(fpr_svm, tpr_svm, color='red', alpha=0.3)\n",
    "axes[0, 1].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "axes[0, 1].set_xlim([0.0, 1.0])\n",
    "axes[0, 1].set_ylim([0.0, 1.05])\n",
    "axes[0, 1].set_xlabel('False Positive Rate (FPR)')\n",
    "axes[0, 1].set_ylabel('True Positive Rate (TPR)')\n",
    "axes[0, 1].set_title('SVM ROC Curve')\n",
    "axes[0, 1].legend(loc='lower right')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "axes[1, 0].plot(fpr_rf, tpr_rf, color='green', lw=2, label='Random Forest (AUC = %0.2f)' % roc_auc_rf)\n",
    "axes[1, 0].fill_between(fpr_rf, tpr_rf, color='green', alpha=0.3)\n",
    "axes[1, 0].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "axes[1, 0].set_xlim([0.0, 1.0])\n",
    "axes[1, 0].set_ylim([0.0, 1.05])\n",
    "axes[1, 0].set_xlabel('False Positive Rate (FPR)')\n",
    "axes[1, 0].set_ylabel('True Positive Rate (TPR)')\n",
    "axes[1, 0].set_title('Random Forest ROC Curve')\n",
    "axes[1, 0].legend(loc='lower right')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].plot(fpr_dt, tpr_dt, color='orange', lw=2, label='Decision Tree (AUC = %0.2f)' % roc_auc_dt)\n",
    "axes[1, 1].fill_between(fpr_dt, tpr_dt, color='orange', alpha=0.3)\n",
    "axes[1, 1].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "axes[1, 1].set_xlim([0.0, 1.0])\n",
    "axes[1, 1].set_ylim([0.0, 1.05])\n",
    "axes[1, 1].set_xlabel('False Positive Rate (FPR)')\n",
    "axes[1, 1].set_ylabel('True Positive Rate (TPR)')\n",
    "axes[1, 1].set_title('Decision Tree ROC Curve')\n",
    "axes[1, 1].legend(loc='lower right')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "axes[2, 0].plot(fpr_nb, tpr_nb, color='purple', lw=2, label='Naive Bayes (AUC = %0.2f)' % roc_auc_nb)\n",
    "axes[2, 0].fill_between(fpr_nb, tpr_nb, color='purple', alpha=0.3)\n",
    "axes[2, 0].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "axes[2, 0].set_xlim([0.0, 1.0])\n",
    "axes[2, 0].set_ylim([0.0, 1.05])\n",
    "axes[2, 0].set_xlabel('False Positive Rate (FPR)')\n",
    "axes[2, 0].set_ylabel('True Positive Rate (TPR)')\n",
    "axes[2, 0].set_title('Naive Bayes ROC Curve')\n",
    "axes[2, 0].legend(loc='lower right')\n",
    "axes[2, 0].grid(True)\n",
    "\n",
    "axes[2, 1].plot(fpr_mlp, tpr_mlp, color='brown', lw=2, label='MLP (AUC = %0.2f)' % roc_auc_mlp)\n",
    "axes[2, 1].fill_between(fpr_mlp, tpr_mlp, color='brown', alpha=0.3)\n",
    "axes[2, 1].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "axes[2, 1].set_xlim([0.0, 1.0])\n",
    "axes[2, 1].set_ylim([0.0, 1.05])\n",
    "axes[2, 1].set_xlabel('False Positive Rate (FPR)')\n",
    "axes[2, 1].set_ylabel('True Positive Rate (TPR)')\n",
    "axes[2, 1].set_title('MLP ROC Curve')\n",
    "axes[2, 1].legend(loc='lower right')\n",
    "axes[2, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff00ac73",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; color:#141140; text-indent: 20px; line-height: 1.5em;\">\n",
    "The AUC (Area Under the Curve) is a metric used to evaluate the performance of binary classification models. When AUC = 0.5, it implies that the model performs no better than random guessing, indicating no discrimination ability. On the other hand, an AUC of 1 suggests perfect discrimination, meaning the model can accurately distinguish between positive and negative instances. AUC values greater than 0.5 but less than 1 indicate better-than-random discrimination, with higher values representing better overall model performance in terms of its ability to separate the classes. </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0235f842",
   "metadata": {},
   "source": [
    "<a id=\"regression\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 12 | Modeling (Regression)</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07669766",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>üåü Explanation of Huber Regression üåü</b></h1>\n",
    "    <ol style=\"font-size:20px;line-height: 1.5em; font-family:calibri; padding-left: 20px;\">\n",
    "        <li>\n",
    "            <b>What is Huber Regression?</b><br>\n",
    "            Huber regression is a robust regression technique used to model relationships between variables. It is particularly effective in handling outliers in the data. Unlike ordinary least squares (OLS) regression, which minimizes the squared difference between predicted and actual values, Huber regression minimizes a combination of squared errors and absolute errors. This makes it less sensitive to outliers and provides more reliable estimates of the model parameters.\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>How does Huber Regression handle outliers?</b><br>\n",
    "            In Huber regression, the loss function used for optimization includes both squared errors and absolute errors. The squared error term penalizes deviations within a certain range, controlled by a parameter called \\( \\delta \\). When the residual (the difference between predicted and actual values) is within this range, the squared error term is used to calculate the loss. However, if the residual exceeds \\( \\delta \\), the absolute error term is used instead. This adaptive approach allows Huber regression to effectively handle outliers by providing a robust estimation of the error.\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>How does Huber Regression identify outliers?</b><br>\n",
    "            During the training process, Huber regression adjusts the model parameters (coefficients) to minimize the combined loss function, which includes both squared and absolute errors. By iteratively updating the parameters based on the observed residuals, the model identifies outliers and adjusts its estimation accordingly. This iterative optimization process allows Huber regression to identify and mitigate the influence of outliers on the model's predictions.\n",
    "        </li>\n",
    "        <li>\n",
    "            <b>What are the advantages of Huber Regression?</b><br>\n",
    "            Huber regression offers several advantages over traditional regression techniques:\n",
    "            <ul>\n",
    "                <li>Robustness to outliers: Huber regression provides robust estimates of the model parameters by incorporating both squared and absolute errors in the loss function.</li>\n",
    "                <li>Improved performance: By handling outliers more effectively, Huber regression often leads to improved model performance and better predictions on real-world datasets.</li>\n",
    "                <li>Flexibility: The \\( \\delta \\) parameter in Huber regression allows users to control the sensitivity to outliers, providing flexibility in modeling different types of data.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb0ccc",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAC6Fd; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:20px; font-family:calibri; color:#141140;\"><b>Huber Loss Function:</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c33b05",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <div style=\"font-size:18px; font-family:calibri; color:#141140;\">\n",
    "        <p>The Huber loss function, denoted as \\(L_{\\delta}(r)\\), is used in Huber regression to calculate the loss between the predicted and actual values.</p>\n",
    "        <p>For \\(|r| \\leq \\delta\\), where \\(r\\) is the residual (the difference between the predicted and actual values), the loss is calculated as:</p>\n",
    "        <p style=\"margin-left: 20px;\"><b>\\(L_{\\delta}(r) = \\frac{1}{2} r^2\\)</b></p>\n",
    "        <p>This means that when the residual is within a certain range (controlled by the parameter \\(\\delta\\)), the loss is proportional to the squared value of the residual, providing a smooth transition from linear to quadratic penalty.</p>\n",
    "        <p>For \\(|r| > \\delta\\), when the residual exceeds the specified range, the loss is calculated as:</p>\n",
    "        <p style=\"margin-left: 20px;\"><b>\\(L_{\\delta}(r) = \\delta(|r| - \\frac{1}{2}\\delta)\\)</b></p>\n",
    "        <p>In this case, the loss is a linear function of the residual with a slope of \\(\\delta\\). This component penalizes larger residuals more than the squared error term and helps in handling outliers more effectively.</p>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = df.drop([\"live\"], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_scale = df_reg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c819e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca625d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_scale.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a95d88",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140; line-height: 1.5em;\">L2 normalization is valuable in Huber regression as it standardizes feature scales, ensuring each feature contributes equally to the model's optimization. This is crucial in balancing the robustness to outliers and the efficiency of error minimization. Additionally, L2 normalization helps mitigate the impact of outliers by reducing their influence on the optimization process, leading to more stable and reliable model fitting.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ecdad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalization(data):\n",
    "    norms = np.linalg.norm(data, axis=1, ord=2, keepdims=True)\n",
    "    normalized_data = data / norms\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_scale_reg = [\"dose_rate\", \"stage\", \"EQD2_HRCTV\", \"EQ_REC\", \"EQ_BLD\", \"EQ_SIG\", \"POINT_A_L\", \"POINT_A_R\", \"VOLUM\", \"TX_duration\", \"age\", \"chemo\", \"marrage\", \"surgery\", \"smoke\"]\n",
    "\n",
    "features_subset_reg = df_reg[features_to_scale_reg]\n",
    "\n",
    "normalized_features_reg = l2_normalization(features_subset_reg)\n",
    "\n",
    "df_reg[features_to_scale_reg] = normalized_features_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcaebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reg = df_reg.drop(\"days\", axis=1)\n",
    "y_reg = df_reg[\"days\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb74ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c80a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ns = df_no_scale.drop(\"days\", axis=1)\n",
    "y_ns = df_no_scale[\"days\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ns_train, X_ns_test, y_ns_train, y_ns_test = train_test_split(X_ns, y_ns, test_size=0.3, random_state=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf6cd5",
   "metadata": {},
   "source": [
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Huber Regression</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_reg = HuberRegressor(epsilon=1.35)\n",
    "huber_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "y_train_pred = huber_reg.predict(X_reg_train)\n",
    "\n",
    "mse_train = mean_squared_error(y_reg_train, y_train_pred)\n",
    "r2_train = r2_score(y_reg_train, y_train_pred)\n",
    "print(\"Training - Mean Squared Error:\", mse_train)\n",
    "print(\"Training - R-squared (R2) Score:\", r2_train)\n",
    "\n",
    "y_test_pred = huber_reg.predict(X_reg_test)\n",
    "\n",
    "mse_test = mean_squared_error(y_reg_test, y_test_pred)\n",
    "r2_test = r2_score(y_reg_test, y_test_pred)\n",
    "print(\"Test - Mean Squared Error:\", mse_test)\n",
    "print(\"Test - R-squared (R2) Score:\", r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_reg.drop(\"days\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53170f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hub = huber_reg.predict(df_new)\n",
    "\n",
    "outliers_mask = (abs(y_reg - y_pred_hub) < 500) # Epxplanation in description below\n",
    "X_filtered = df_new[outliers_mask]\n",
    "y_filtered = y_reg[outliers_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b966c7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140; line-height: 1.5em;\"> outliers_mask = (abs(y_reg - y_pred_hub) &lt; 500) <br> This line calculates a mask indicating whether each data point is considered an outlier based on the absolute difference between the actual target values (<code>y_reg</code>) and the predicted values (<code>y_pred_hub</code>). \n",
    "\n",
    "Let's break down the expression:\n",
    "\n",
    "- <code>abs(y_reg - y_pred_hub)</code>: This calculates the absolute difference between the actual target values (<code>y_reg</code>) and the predicted values (<code>y_pred_hub</code>). Absolute difference is used to ensure that both positive and negative deviations from the prediction are considered.\n",
    "  \n",
    "- <code>&lt; 500</code>: This condition checks if the absolute difference for each data point is less than 500. If the absolute difference is less than 500, it means that the prediction is within 500 days of the actual value.\n",
    "\n",
    "- <code>outliers_mask</code>: This variable stores a boolean mask where each element is <code>True</code> if the corresponding data point is not considered an outlier (i.e., the absolute difference is less than 500 days), and <code>False</code> if it is considered an outlier.\n",
    "\n",
    "In summary, this line creates a mask to identify outliers by checking if the absolute difference between the actual and predicted values for each data point is less than 500 days.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42cd880",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23892f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc69310",
   "metadata": {},
   "source": [
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">C-Index</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ffe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d521e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_censored\n",
    "from sklearn import set_config\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "\n",
    "new_y = df[[\"live\", \"days\"]][outliers_mask]\n",
    "\n",
    "set_config(display=\"text\")\n",
    "\n",
    "\n",
    "structured_y = np.array(list(zip(new_y[\"live\"].astype(bool),\n",
    "                                 new_y[\"days\"])),\n",
    "                        dtype=[('event', bool), ('time', float)])\n",
    "\n",
    "estimator = CoxPHSurvivalAnalysis()\n",
    "estimator.fit(X_filtered, structured_y)\n",
    "\n",
    "prediction = estimator.predict(X_filtered)\n",
    "\n",
    "result = concordance_index_censored(new_y[\"live\"].astype(bool), new_y[\"days\"], prediction)\n",
    "print(\"Concordance Index:\", result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc44cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b08a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hub_train, X_hub_test, y_hub_train, y_hub_test = train_test_split(X_filtered ,y_filtered, test_size=0.3, random_state=31)\n",
    "\n",
    "huber_reg.fit(X_hub_train, y_hub_train)\n",
    "y_pred_filtered = huber_reg.predict(X_hub_test)\n",
    "\n",
    "mse_filtered = mean_squared_error(y_hub_test, y_pred_filtered)\n",
    "print(f\"Mean Squared Error after removing outliers: {mse_filtered}\")\n",
    "\n",
    "r2 = r2_score(y_hub_test, y_pred_filtered)\n",
    "print(\"R-squared (R2) after removing outliers:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "huber_reg = HuberRegressor(epsilon=1.35)\n",
    "\n",
    "huber_reg.fit(X_hub_train, y_hub_train)\n",
    "\n",
    "y_train_pred = huber_reg.predict(X_hub_train)\n",
    "y_test_pred = huber_reg.predict(X_hub_test)\n",
    "\n",
    "r2_train = r2_score(y_hub_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_hub_train, y_train_pred)\n",
    "print(\"Training - R2 Score:\", r2_train)\n",
    "print(\"Training - Mean Squared Error:\", mse_train)\n",
    "\n",
    "r2_test = r2_score(y_hub_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_hub_test, y_test_pred)\n",
    "print(\"Test - R2 Score:\", r2_test)\n",
    "print(\"Test - Mean Squared Error:\", mse_test)\n",
    "\n",
    "cv_r2_scores = cross_val_score(huber_reg, X_hub_train, y_hub_train, cv=5, scoring='r2', n_jobs=-1)\n",
    "print(\"Cross-Validation - R2 Scores:\", cv_r2_scores)\n",
    "\n",
    "mean_cv_r2_score = cv_r2_scores.mean()\n",
    "print(\"Mean Cross-Validation - R2 Score:\", mean_cv_r2_score)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "axs[0].scatter(y_hub_train, y_train_pred, color='blue', label='Predicted (Training)')\n",
    "axs[0].plot([min(y_hub_train), max(y_hub_train)], [min(y_hub_train), max(y_hub_train)], color='red', linestyle='--')\n",
    "axs[0].set_title('Actual vs Predicted (Training)')\n",
    "axs[0].set_xlabel('Actual Values')\n",
    "axs[0].set_ylabel('Predicted Values')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].scatter(y_hub_test, y_test_pred, color='green', label='Predicted (Test)')\n",
    "axs[1].plot([min(y_hub_test), max(y_hub_test)], [min(y_hub_test), max(y_hub_test)], color='red', linestyle='--')\n",
    "axs[1].set_title('Actual vs Predicted (Test)')\n",
    "axs[1].set_xlabel('Actual Values')\n",
    "axs[1].set_ylabel('Predicted Values')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Before', 'After']\n",
    "mse_train_values = [323632.000285592, 48811.730381685404]\n",
    "mse_test_values = [415624.5479774113, 56325.05777718468]\n",
    "r2_train_values = [0.42059802296035453, 0.8826983215504424]\n",
    "r2_test_values = [0.2277225373603774, 0.8539311574552941]\n",
    "colors = ['#B656AF', '#D166B8']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(labels))\n",
    "\n",
    "plt.bar(index - bar_width/2, mse_train_values, bar_width, color=colors[0], label='Train')\n",
    "plt.bar(index + bar_width/2, mse_test_values, bar_width, color=colors[1], label='Test')\n",
    "for i, value in enumerate(mse_train_values):\n",
    "    plt.text(i - bar_width/2, value, f'{value:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "for i, value in enumerate(mse_test_values):\n",
    "    plt.text(i + bar_width/2, value, f'{value:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('Outlier Removal')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('Mean Squared Error (MSE) before and after Outlier Removal')\n",
    "plt.xticks(index, labels)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.bar(index - bar_width/2, r2_train_values, bar_width, color=colors[0], label='Train')\n",
    "plt.bar(index + bar_width/2, r2_test_values, bar_width, color=colors[1], label='Test')\n",
    "for i, value in enumerate(r2_train_values):\n",
    "    plt.text(i - bar_width/2, value, f'{value:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "for i, value in enumerate(r2_test_values):\n",
    "    plt.text(i + bar_width/2, value, f'{value:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('Outlier Removal')\n",
    "plt.ylabel('R-squared (R2) Score')\n",
    "plt.title('R-squared (R2) Score before and after Outlier Removal')\n",
    "plt.xticks(index, labels)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f11b6",
   "metadata": {},
   "source": [
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Linear Regression</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1287c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "\n",
    "lin_reg.fit(X_hub_train, y_hub_train)\n",
    "\n",
    "y_train_pred = lin_reg.predict(X_hub_train)\n",
    "\n",
    "r2_train = r2_score(y_hub_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_hub_train, y_train_pred)\n",
    "print(\"Training - R2 Score:\", r2_train)\n",
    "print(\"Training - Mean Squared Error:\", mse_train)\n",
    "\n",
    "y_test_pred = lin_reg.predict(X_hub_test)\n",
    "\n",
    "r2_test = r2_score(y_hub_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_hub_test, y_test_pred)\n",
    "print(\"Test - R2 Score:\", r2_test)\n",
    "print(\"Test - Mean Squared Error:\", mse_test)\n",
    "\n",
    "cv_r2_scores = cross_val_score(lin_reg, X_hub_train, y_hub_train, cv=5, scoring='r2')\n",
    "print(\"Cross-Validation - R2 Scores:\", cv_r2_scores)\n",
    "\n",
    "mean_cv_r2_score = cv_r2_scores.mean()\n",
    "print(\"Mean Cross-Validation - R2 Score:\", mean_cv_r2_score)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "axs[0].scatter(y_hub_train, y_train_pred, color='blue', label='Predicted (Training)')\n",
    "axs[0].plot([min(y_hub_train), max(y_hub_train)], [min(y_hub_train), max(y_hub_train)], color='red', linestyle='--')\n",
    "axs[0].set_title('Actual vs Predicted (Training)')\n",
    "axs[0].set_xlabel('Actual Values')\n",
    "axs[0].set_ylabel('Predicted Values')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].scatter(y_hub_test, y_test_pred, color='green', label='Predicted (Test)')\n",
    "axs[1].plot([min(y_hub_test), max(y_hub_test)], [min(y_hub_test), max(y_hub_test)], color='red', linestyle='--')\n",
    "axs[1].set_title('Actual vs Predicted (Test)')\n",
    "axs[1].set_xlabel('Actual Values')\n",
    "axs[1].set_ylabel('Predicted Values')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b27d5",
   "metadata": {},
   "source": [
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Support Vector Regressor</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f431e63",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140; line-height: 1.5em;\">\n",
    "        The <b>epsilon</b> parameter in Support Vector Regression (SVR) determines the margin of tolerance where no penalty is given to errors that are within this margin. It defines the tube around the predicted values within which no penalty is associated with the errors.\n",
    "        Adjusting the epsilon parameter can have a significant impact on the flexibility of the SVR model and its ability to capture the underlying patterns in the data.\n",
    "        Therefore, selecting an appropriate epsilon value involves balancing the trade-off between model flexibility and generalization performance, and it often requires experimentation and validation with cross-validation techniques.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd466eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000, 50000],\n",
    "    'epsilon': [0.1, 0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_hub_train, y_hub_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "y_train_pred = grid_search.predict(X_hub_train)\n",
    "r2_train = r2_score(y_hub_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_hub_train, y_train_pred)\n",
    "print(\"Training - R2 Score:\", r2_train)\n",
    "print(\"Training - Mean Squared Error:\", mse_train)\n",
    "\n",
    "y_test_pred = grid_search.predict(X_hub_test)\n",
    "r2_test = r2_score(y_hub_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_hub_test, y_test_pred)\n",
    "print(\"Test - R2 Score:\", r2_test)\n",
    "print(\"Test - Mean Squared Error:\", mse_test)\n",
    "\n",
    "cv_r2_scores = cross_val_score(grid_search.best_estimator_, X_hub_train, y_hub_train, cv=5, scoring='r2')\n",
    "print(\"Cross-Validation - R2 Scores:\", cv_r2_scores)\n",
    "\n",
    "mean_cv_r2_score = cv_r2_scores.mean()\n",
    "print(\"Mean Cross-Validation - R2 Score:\", mean_cv_r2_score)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "axs[0].scatter(y_hub_train, y_train_pred, color='blue', label='Predicted (Training)')\n",
    "axs[0].plot([min(y_hub_train), max(y_hub_train)], [min(y_hub_train), max(y_hub_train)], color='red', linestyle='--')\n",
    "axs[0].set_title('Actual vs Predicted (Training)')\n",
    "axs[0].set_xlabel('Actual Values')\n",
    "axs[0].set_ylabel('Predicted Values')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].scatter(y_hub_test, y_test_pred, color='green', label='Predicted (Test)')\n",
    "axs[1].plot([min(y_hub_test), max(y_hub_test)], [min(y_hub_test), max(y_hub_test)], color='red', linestyle='--')\n",
    "axs[1].set_title('Actual vs Predicted (Test)')\n",
    "axs[1].set_xlabel('Actual Values')\n",
    "axs[1].set_ylabel('Predicted Values')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb2be8",
   "metadata": {},
   "source": [
    "# <p style=\"background-color: #9F2B68; font-family:calibri; color:#E6E6FA; font-size:20px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:10px;\">Random Forest Regressor</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8411ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
    "    'max_depth': [2],\n",
    "}\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=31)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_hub_train, y_hub_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "best_rf_reg = RandomForestRegressor(**best_params, random_state=31)  # Set random_state here too\n",
    "best_rf_reg.fit(X_hub_train, y_hub_train)\n",
    "\n",
    "y_train_pred = best_rf_reg.predict(X_hub_train)\n",
    "\n",
    "r2_train = r2_score(y_hub_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_hub_train, y_train_pred)\n",
    "print(\"Training - R2 Score:\", r2_train)\n",
    "print(\"Training - Mean Squared Error:\", mse_train)\n",
    "\n",
    "y_test_pred = best_rf_reg.predict(X_hub_test)\n",
    "\n",
    "r2_test = r2_score(y_hub_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_hub_test, y_test_pred)\n",
    "print(\"Test - R2 Score:\", r2_test)\n",
    "print(\"Test - Mean Squared Error:\", mse_test)\n",
    "\n",
    "cv_r2_scores = cross_val_score(best_rf_reg, X_hub_train, y_hub_train, cv=5, scoring='r2', n_jobs=-1)\n",
    "print(\"Cross-Validation - R2 Scores:\", cv_r2_scores)\n",
    "\n",
    "mean_cv_r2_score = cv_r2_scores.mean()\n",
    "print(\"Mean Cross-Validation - R2 Score:\", mean_cv_r2_score)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "axs[0].scatter(y_hub_train, y_train_pred, color='blue', label='Predicted (Training)')\n",
    "axs[0].plot([min(y_hub_train), max(y_hub_train)], [min(y_hub_train), max(y_hub_train)], color='red', linestyle='--')\n",
    "axs[0].set_title('Actual vs Predicted (Training)')\n",
    "axs[0].set_xlabel('Actual Values')\n",
    "axs[0].set_ylabel('Predicted Values')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].scatter(y_hub_test, y_test_pred, color='green', label='Predicted (Test)')\n",
    "axs[1].plot([min(y_hub_test), max(y_hub_test)], [min(y_hub_test), max(y_hub_test)], color='red', linestyle='--')\n",
    "axs[1].set_title('Actual vs Predicted (Test)')\n",
    "axs[1].set_xlabel('Actual Values')\n",
    "axs[1].set_ylabel('Predicted Values')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7ec68",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "<h1 style=\"font-size:24px; font-family:calibri; color:#141140; text-align:center;\"><b>üåü Conclusion of Regression Models</b> üåü</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30640bfb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <ol style=\"font-size:18px; font-family:calibri; color:#141140; line-height: 1.5em;\">\n",
    "        <li>\n",
    "            <strong>Huber Regression:</strong>\n",
    "            <ul>\n",
    "                <li>Training - R2 Score: 0.8827</li>\n",
    "                <li>Training - Mean Squared Error: 48811.73</li>\n",
    "                <li>Test - R2 Score: 0.8539</li>\n",
    "                <li>Test - Mean Squared Error: 56325.06</li>\n",
    "                <li>Mean Cross-Validation - R2 Score: 0.8584</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>\n",
    "            <strong>Linear Regression:</strong>\n",
    "            <ul>\n",
    "                <li>Training - R2 Score: 0.9032</li>\n",
    "                <li>Training - Mean Squared Error: 40275.55</li>\n",
    "                <li>Test - R2 Score: 0.8252</li>\n",
    "                <li>Test - Mean Squared Error: 67387.91</li>\n",
    "                <li>Mean Cross-Validation - R2 Score: 0.7307</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>\n",
    "            <strong>SVR:</strong>\n",
    "            <ul>\n",
    "                <li>Best Parameters: {'C': 50000, 'epsilon': 0.1}</li>\n",
    "                <li>Training - R2 Score: 0.9282</li>\n",
    "                <li>Training - Mean Squared Error: 29865.72</li>\n",
    "                <li>Test - R2 Score: 0.9031</li>\n",
    "                <li>Test - Mean Squared Error: 37357.94</li>\n",
    "                <li>Mean Cross-Validation - R2 Score: 0.8685</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>\n",
    "            <strong>Random Forest Regressor:</strong>\n",
    "            <ul>\n",
    "                <li>Best Parameters: {'max_depth': 2, 'n_estimators': 15}</li>\n",
    "                <li>Training - R2 Score: 0.9005</li>\n",
    "                <li>Training - Mean Squared Error: 41395.43</li>\n",
    "                <li>Test - R2 Score: 0.8515</li>\n",
    "                <li>Test - Mean Squared Error: 57256.85</li>\n",
    "                <li>Mean Cross-Validation - R2 Score: 0.8359</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d8ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_values2 = {\n",
    "    'Huber Regression': {'train': 88, 'test': 85},\n",
    "    'Linear Regression': {'train': 90, 'test': 82.5},\n",
    "    'SVR': {'train': 93, 'test': 90},\n",
    "    'Random Forest Regression': {'train': 90, 'test': 85},\n",
    "}\n",
    "\n",
    "sorted_values = sorted(train_test_values2.items(), key=lambda x: x[1]['train'], reverse=True)\n",
    "algorithms = [item[0] for item in sorted_values]\n",
    "train_accuracies = [item[1]['train'] for item in sorted_values]\n",
    "test_accuracies = [item[1]['test'] for item in sorted_values]\n",
    "\n",
    "bar_height = 0.35\n",
    "index = np.arange(len(algorithms))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.barh(index, train_accuracies, bar_height, label='Train', color='#880085')\n",
    "plt.barh(index + bar_height, test_accuracies, bar_height, label='Test', color='#FF00FF')\n",
    "\n",
    "for i, (train_acc, test_acc) in enumerate(zip(train_accuracies, test_accuracies)):\n",
    "    plt.text(train_acc + 1, i, str(train_acc), ha='left', va='center', color='black')\n",
    "    plt.text(test_acc + 1, i + bar_height, str(test_acc), ha='left', va='center', color='black')\n",
    "\n",
    "plt.xlabel('R2 Score')\n",
    "plt.ylabel('Regression Algorithm')\n",
    "plt.title('Train and Test R2 Scores  of Different Regression Algorithms')\n",
    "plt.yticks(index + bar_height / 2, algorithms)\n",
    "plt.gca().invert_yaxis() \n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287562f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = {\n",
    "    'Huber Regression': {'train': 48811.73, 'test': 56325.06},\n",
    "    'Linear Regression': {'train': 40275.55, 'test': 67387.91},\n",
    "    'SVR': {'train': 29865.72, 'test': 37357.94},\n",
    "    'Random Forest Regression': {'train': 41395.43, 'test': 57256.85},\n",
    "}\n",
    "\n",
    "sorted_values = sorted(MSE.items(), key=lambda x: x[1]['train'], reverse=True)\n",
    "algorithms = [item[0] for item in sorted_values]\n",
    "train_accuracies = [item[1]['train'] for item in sorted_values]\n",
    "test_accuracies = [item[1]['test'] for item in sorted_values]\n",
    "\n",
    "bar_height = 0.35\n",
    "index = np.arange(len(algorithms))\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.barh(index, train_accuracies, bar_height, label='Train', color='#880085')\n",
    "plt.barh(index + bar_height, test_accuracies, bar_height, label='Test', color='#FF00FF')\n",
    "\n",
    "for i, (train_acc, test_acc) in enumerate(zip(train_accuracies, test_accuracies)):\n",
    "    plt.text(train_acc + 1, i, str(train_acc), ha='left', va='center', color='black')\n",
    "    plt.text(test_acc + 1, i + bar_height, str(test_acc), ha='left', va='center', color='black')\n",
    "\n",
    "plt.xlabel('MSE')\n",
    "plt.ylabel('Regression Algorithm')\n",
    "plt.title('Train and Test MSE of Different Regression Algorithms')\n",
    "plt.yticks(index + bar_height / 2, algorithms)\n",
    "plt.gca().invert_yaxis() \n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_values = {\n",
    "    'Huber Regression': 86,\n",
    "    'Linear Regression': 73, \n",
    "    'SVR': 87,\n",
    "    'Random Forest Regression': 84,\n",
    "}\n",
    "\n",
    "sorted_values = sorted(cross_values.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "algorithms = [item[0] for item in sorted_values]\n",
    "auc_scores = [item[1] for item in sorted_values]\n",
    "\n",
    "colors = ['#880085', '#FF00FF', '#6A0DAD', '#B656AF', '#D166B8', '#F99BB0']\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "bars = plt.barh(algorithms[::-1], auc_scores[::-1], color=colors)\n",
    "plt.xlabel('Cross Validation')\n",
    "plt.ylabel('Regression Algorithm')\n",
    "plt.title('Comparison of Cross-validation of Different Regression Algorithms')\n",
    "plt.grid(axis='x')  \n",
    "plt.xlim(0, 100)  \n",
    "\n",
    "for bar, score in zip(bars, auc_scores[::-1]):\n",
    "    plt.text(score + 1, bar.get_y() + bar.get_height() / 2, f'{score}%', \n",
    "             va='center', ha='left', color='black', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f727b6d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140; line-height: 1.5em;\">\n",
    "        <strong>Comparison and Explanation:</strong>\n",
    "        <ul style=\"font-size:18px; font-family:calibri; color:#141140; line-height: 1.5em;\">\n",
    "            <li><strong>Training Performance:</strong>\n",
    "                <ul>\n",
    "                    <li>SVR has the highest R2 score (0.9282), indicating the best fit to the training data, closely followed by Linear Regression (0.9032) and Random Forest Regressor (0.9005). Huber Regression has a slightly lower R2 score of 0.8827.</li>\n",
    "                    <li>SVR also has the lowest mean squared error on the training set (29865.72), indicating the best performance in minimizing prediction errors during training.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li><strong>Test Performance:</strong>\n",
    "                <ul>\n",
    "                    <li>SVR also performs the best on the test set with an R2 score of 0.9031 and the lowest mean squared error (37357.94), suggesting that it generalizes well to unseen data.</li>\n",
    "                    <li>Linear Regression has the lowest R2 score (0.8252) and the highest mean squared error (67387.91) among the models on the test set, indicating poorer performance compared to the others.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li><strong>Cross-Validation Performance:</strong>\n",
    "    <ul>\n",
    "        <li>SVR exhibits the highest mean cross-validation R2 score (0.8685), indicating its robustness and consistency across different folds.</li>\n",
    "        <li>Huber Regression performs comparably well in cross-validation with a mean R2 score of 0.8584, indicating its effectiveness in capturing the underlying patterns in the data.</li>\n",
    "        <li>Random Forest Regressor also shows good performance in cross-validation with a mean R2 score of 0.8359.</li>\n",
    "        <li>Linear Regression, however, has a relatively lower mean R2 score of 0.7307 in cross-validation, suggesting poorer performance compared to the other models.</li>\n",
    "    </ul>\n",
    "</li>\n",
    "        </ul>\n",
    "        <p style=\"font-size:18px; font-family:calibri; color:#141140; line-height: 1.5em;\"><strong>Overall, SVR with the specified parameters appears to be the best-performing model, exhibiting strong performance in both training, testing, and cross-validation.</strong></p>\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64884b8",
   "metadata": {},
   "source": [
    "<a id=\"conc\"></a>\n",
    "# <p style=\"background-color: #B0306A; font-family:calibri; color:white; font-size:30px; font-family:Calibri; text-align:center; border-radius:15px 50px; padding:15px;\">Step 13 | Conclusion</p>\n",
    "‚¨ÜÔ∏è [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5add4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140; line-height: 1.5em;\">Comparing Cross validation scores for all algorithms:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "auc_values = {\n",
    "    'SVM, kernel=\"poly\", C=0.7, degree=3': 72,\n",
    "    'KNN, k=5, p=1': 68, \n",
    "    'MLP, hidden_layer_sizes=100, max_iter=150, alpha=0.5': 63.5,\n",
    "    'RF, criterion=\"gini\", max_depth=5, n_estimators=12': 66,\n",
    "    'GaussianNB': 56,\n",
    "    'DT, criterion=\"entropy\", max_depth=4': 61\n",
    "}\n",
    "\n",
    "sorted_values = sorted(auc_values.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "algorithms = [item[0] for item in sorted_values]\n",
    "auc_scores = [item[1] for item in sorted_values]\n",
    "\n",
    "colors = ['#880085', '#FF00FF', '#6A0DAD', '#B656AF', '#D166B8', '#F99BB0']\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "bars = plt.barh(algorithms[::-1], auc_scores[::-1], color=colors)\n",
    "plt.xlabel('Cross Validation')\n",
    "plt.ylabel('Algorithm')\n",
    "plt.title('Comparison of Cross-validation of Different Algorithms')\n",
    "plt.grid(axis='x')  \n",
    "plt.xlim(0, 100)  \n",
    "\n",
    "for bar, score in zip(bars, auc_scores[::-1]):\n",
    "    plt.text(score + 1, bar.get_y() + bar.get_height() / 2, f'{score}%', \n",
    "             va='center', ha='left', color='black', fontsize=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea6ab1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140; line-height: 1.5em;\">Comparing AUC (C-Index) for all algorithms:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_values = {\n",
    "    'MLP, hidden_layer_sizes=100, max_iter=150, alpha=0.5': 76,\n",
    "    'KNN, k=5, p=1': 73,\n",
    "    'RF, criterion=\"gini\", max_depth=5, n_estimators=12': 71,\n",
    "    'SVM, kernel=\"poly\", C=0.7, degree=3': 84,\n",
    "    'DT, criterion=\"entropy\", max_depth=4': 67,\n",
    "    'GaussianNB': 69\n",
    "}\n",
    "\n",
    "sorted_values = sorted(auc_values.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "algorithms = [item[0] for item in sorted_values]\n",
    "auc_scores = [item[1] for item in sorted_values]\n",
    "\n",
    "colors = ['#880085', '#FF00FF', '#6A0DAD', '#B656AF', '#D166B8', '#F99BB0']\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "bars = plt.barh(algorithms[::-1], auc_scores[::-1], color=colors)\n",
    "plt.xlabel('AUC')\n",
    "plt.ylabel('Algorithm')\n",
    "plt.title('Comparison of AUC of Different Algorithms')\n",
    "plt.grid(axis='x')  \n",
    "plt.xlim(0, 100)  \n",
    "\n",
    "for bar, score in zip(bars, auc_scores[::-1]):\n",
    "    plt.text(score + 1, bar.get_y() + bar.get_height() / 2, f'{score}', \n",
    "             va='center', ha='left', color='black', fontsize=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67863f9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140; line-height: 1.5em;\">Comparing Train and Test Accuracies for all algorithms:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a3101",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_values = {\n",
    "    'MLP, hidden_layer_sizes=100, max_iter=150, alpha=0.5': {'train': 88, 'test': 65},\n",
    "    'KNN, k=5, p=1': {'train': 77, 'test': 63},\n",
    "    'RF, criterion=\"gini\", max_depth=5, n_estimators=12': {'train': 84, 'test': 70},\n",
    "    'SVM, kernel=\"poly\", C=0.7, degree=3': {'train': 79, 'test': 65},\n",
    "    'DT, criterion=\"entropy\", max_depth=4': {'train': 76, 'test': 60},\n",
    "    'GaussianNB': {'train': 69, 'test': 60}\n",
    "}\n",
    "\n",
    "sorted_values = sorted(train_test_values.items(), key=lambda x: x[1]['train'], reverse=True)\n",
    "algorithms = [item[0] for item in sorted_values]\n",
    "train_accuracies = [item[1]['train'] for item in sorted_values]\n",
    "test_accuracies = [item[1]['test'] for item in sorted_values]\n",
    "\n",
    "bar_height = 0.35\n",
    "index = np.arange(len(algorithms))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.barh(index, train_accuracies, bar_height, label='Train', color='#880085')\n",
    "plt.barh(index + bar_height, test_accuracies, bar_height, label='Test', color='#FF00FF')\n",
    "\n",
    "for i, (train_acc, test_acc) in enumerate(zip(train_accuracies, test_accuracies)):\n",
    "    plt.text(train_acc + 1, i, str(train_acc), ha='left', va='center', color='black')\n",
    "    plt.text(test_acc + 1, i + bar_height, str(test_acc), ha='left', va='center', color='black')\n",
    "\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Algorithm')\n",
    "plt.title('Train and Test Accuracies of Different Algorithms')\n",
    "plt.yticks(index + bar_height / 2, algorithms)\n",
    "plt.gca().invert_yaxis()  \n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74bdac",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; color:#141140; line-height: 1.5em;\">List of top 3 algorithms for 4 main metrics (Accuracy, Precision, Recall, F1-Score) for both classes (0, 1):</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0425723e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FAE6FA; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <ol>\n",
    "        <li><strong>Accuracy</strong>:\n",
    "            <ul>\n",
    "                <li><strong>MLP:</strong> 88% (training), 65% (testing)</li>\n",
    "                <li><strong>RF (Gini criterion):</strong> 84% (training), 70% (testing)</li>\n",
    "                <li><strong>DT (Gini criterion):</strong> 82% (training), 63% (testing)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>Precision (class 0)</strong>:\n",
    "            <ul>\n",
    "                <li><strong>SVM:</strong> 100% (training), 100% (testing)</li>\n",
    "                <li><strong>RF (Gini criterion):</strong> 94% (training), 83% (testing)</li>\n",
    "                <li><strong>MLP:</strong> 88% (training), 75% (testing)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>Precision (class 1)</strong>:\n",
    "            <ul>\n",
    "                <li><strong>MLP:</strong> 84% (training), 61% (testing)</li>\n",
    "                <li><strong>RF (Gini criterion):</strong> 81% (training), 65% (testing)</li>\n",
    "                <li><strong>SVM:</strong> 76% (training), 59% (testing)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>Recall (class 0)</strong>:\n",
    "            <ul>\n",
    "                <li><strong>MLP:</strong> 72% (training), 43% (testing)</li>\n",
    "                <li><strong>RF (Gini criterion):</strong> 53% (training), 48% (testing)</li>\n",
    "                <li><strong>SVM:</strong> 34% (training), 29% (testing)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>Recall (class 1)</strong>:\n",
    "            <ul>\n",
    "                <li><strong>RF (Gini criterion):</strong> 99% (training), 91% (testing)</li>\n",
    "                <li><strong>KNN (p=1):</strong> 97% (training), 95% (testing)</li>\n",
    "                <li><strong>MLP:</strong> 96% (training), 86% (testing)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>F1-score (class 0)</strong>:\n",
    "            <ul>\n",
    "                <li><strong>MLP:</strong> 79% (training), 55% (testing)</li>\n",
    "                <li><strong>RF (Gini criterion):</strong> 68% (training), 61% (testing)</li>\n",
    "                <li><strong>DT (Gini criterion):</strong> 61% (training), 43% (testing)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>F1-score (class 1)</strong>:\n",
    "            <ul>\n",
    "                <li><strong>MLP:</strong> 91% (training), 72% (testing)</li>\n",
    "                <li><strong>RF (Gini criterion):</strong> 89% (training), 75% (testing)</li>\n",
    "                <li><strong>DT (Gini criterion):</strong> 88% (training), 72% (testing)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
